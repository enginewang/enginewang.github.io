[{"categories":["生活"],"content":"第一次开博客应该是在大二的时候，也就是2018年，用的是Hexo。之后觉得配置太少，想要有发挥的空间，就用当时学的Django写了一个博客网站，再后来又改成了Go+Vue，作为一个前后端分离的单体应用，包含博客的完整的功能，还能有其他拓展。\n这本来应该是博客的最终形态了，然而精力有限，不想折腾markdown渲染和其他各种功能，以及为一些前后端、运维、优化的问题所烦恼，再加上服务器成本也不少，还是希望能够纯粹的写作。\n因此现在又回到了最初的起点，转回了静态网页，这次用Hugo搭建，主题用的是Meme，所谓是从简到繁，又从繁到简。\n之前的博客内容会逐步迁移过来，由于静态网页发文章很方便，会增加更多的内容，敬请期待。\n","description":"","tags":["hugo","site"],"title":"返璞归真，博客重新迁移到Hugo","uri":"/posts/hello-world/"},{"categories":["理论"],"content":"拜占庭将军问题 拜占庭将军问题是分布式领域最复杂的一个容错模型，较好地抽象了分布式系统面临的共识问题。\n假如你是一位拜占庭的将军，需要与其他几个国家的军队做沟通，而信使可能会被杀，可能会被替换，可能某国军队会传递错误信息等等，抽象出来的问题就是，如何在可能有错误发生的情况下，让多个节点达成共识，保持一致。\n拜占庭将军是最困难的一种情况，因为会存在恶意节点行为行为，在某些场景（比如数字货币区块链）只能使用拜占庭容错算法（Byzantine Fault Torerace，BFT），常见的拜占庭算法有口信消息型算法、签名消息型算法、PBFT算法、PoW算法等。\n在计算机分布式系统中，最常使用的还是非拜占庭容错算法，也就是故障容错算法（Crash Fault Tolerance，CFT），解决的是分布式系统中存在故障，但不存在恶意节点的场景。常见的算法有Paxos算法、Raft算法、ZAB协议等，这些协议之后都会讲解。\n不过对于恶性的情况，一般只在区块链中出现，算法有PBFT、PoW等。但是我并没有打算涉足区块链相关的研究，所以这些算法不在讨论范围之内。\n共识算法的概念 共识算法就是用来达成一致性的方法。\n需要满足三个条件：\n Termination：保证算法最后可以做出决定，不能是无限循环的 Validity：最终决议一定来自于其中一个参与的节点 Agreement：算法完成时，所有节点一定会做出相同的决定  FLP定理：完美的共识算法不存在。在非同步的网络环境中，就算只应付一个节点故障，也没有一个共识算法能保证完全正确。\n一般的共识算法分为两类：\n Symmetric, no leader  所有的节点地位等同，client可以向每一个server发送请求\nAsymmetric, leader based  任一时刻只会有一个leader，leader处理client的请求，其余server只接受leader的决策，client只可以向leader发送请求。\nPaxos算法 Paxos算法是分布式共识算法的元老，目前最流行的分布式算法都是基于Paxos改进的，所以不得不提。\n兰伯特Lamport提出的Paxos包含两个部分： 一个是Basic Paxos，描述的是多个节点之间如何就一个value达成共识； 一个是Multi-Paxos，描述的是执行多个Basic Paxos实例，为一系列value达成共识。 这一节只说Basic Paxos，Multi-Paxos下一节\n系统角色 提议者（Proposers）：向系统里的其他节点提出v=C，希望大家达成共识。\n接受者（Acceptors）：不发起proposal的节点，接受Proposers的提议。\n学习者（Learner）：不参与投票的过程，被告知投票的结果，接受达成的共识存储保存数据。\n算法流程 分为两个流程：\n第一步：准备阶段\n在提出提案之前，先得到超过半数节点的回应，也就是有半数以上的节点愿意聆听这个Proposer。假设这次要发送的数据是v\n具体的过程：Proposers向所有节点发送Prepare(n)，n包含了一些元信息，可以比较大小，Acceptors接收到后，与这一轮从其他Proposers里收到的最大的提议N比较。\n准备阶段只需要发送n即可，不需要发送v。\n如果$n\u003cN$，也就是目前的这个提议的n比这一轮已有的最大的N还小，直接无视这个提议。 否则，就认为当前提议更好，如果此时已经发送过了一个返回给之前最大的那个Proposer，就返回一个ack(n, (nx, vx))，n是这一次的n，nx是之前最大的那个Proposer的n，vx之前最大的那个Proposer的x。如果之前没接受过其他提议，就发送ack(n, (null, null))\n第二步：接受阶段\nProposers等待过半的Acceptors返回后，对ack作出判断，如果里面有节点的ack返回的nx、vx不为空，就主动放弃，找出里面最大nx的vx，再发送accept(n,vx)给所有的Acceptors。 如果都为空的话，就传送accept(n,v)给所有的Acceptors。\nAcceptors收到accept(n,v)后，不过可能还会收到prepare(n)\nPaxos论文描述： 一些容易想错的地方，进行声明：\n 这里的n并不是具体的int，只是为了简单描述算法，实际上这里的n是一种数据结构，但是相互之间可以被比较，并且对于每个节点而言，它们的n必然不相同。 并不是说一个节点只能当Proposer、Acceptor、Learner中的一种，实际上，每个节点都同时具有这三种角色。 Basic Paxos只是对一个值形成决议，并不是多个值。\n 具体例子 举一个具体的例子，两个客户端作为提议者，n分别为1和5，v分别为3和7，有三个接受者。\n 准备阶段  提议者分别发送Prepare(n)给三个节点，假设说AB先接收到了客户端1的信息，C先接受到了客户端2的信息\n由于接受者之前没有提案（也可以认为目前的n是无穷小），所以接受到第一个提案后都进行响应，返回ack(n, (null, null))\n之后AB接受到客户端2传来的Prepare(5)，5\u003e1，所以会发送准备响应给2。C接受到客户端1传来的Prepare(1)，1\u003c5，直接无视该请求\n接受阶段 由于1、2都收到了多于半数的准备返回响应，并且返回的响应包含的之前最大提案号为空，所以会发送分别接受请求accpet(1, 3)和accept(5, 7)  最后ABC接受到1的确认，由于之前承诺不再接受n小于5的，所以不会变。接受到5的确认后就修改为了5\n假设另一个例子，在某个顺序AB是n=5,v=7，C是n=1,v=3。此时有一个请求[9, 6]，发送给ABC之后，准备阶段，由于9\u003e5，ABC会接受，并返回 ack(n, (nx, nv))，具体是AB返回ack(9, (5, 7)),C返回ack(9, (1, 3))，那么客户端3由于接受到的返回不为空，就会判断之前最大的n，这里是5，对应的v是7，所以在接受阶段会发送accpet(9, 7)给所有节点。\n可以参考这个视频：https://www.youtube.com/watch?v=UUQ8xYWR4do\nMulti-Paxos Multi-Paxos并不是一个具体的算法，而是一种思想。指的是基于Mulit-Paxos算法通过多个Basic Paxos实例实现一系列值的共识的算法。（比如Raft算法、ZAB协议等）\n由于第一阶段收到大多数准备响应的提议者才能发起第二阶段，那么如果多个提议者同时提交提案，可能因为永远无法收到超过半数的准备响应而阻塞。（比如系统中有5个节点，有3个同时发起提案）。 另一个问题是两轮的RPC太消耗性能，也增加了延迟。\n通过引入Leader（领导者）角色以及优化Basic Paxos来解决这两个问题。 Leader节点作为唯一的提议者，这样就不存在提议冲突的情况。 Leader的提案永远是最新的，所以省略掉准备阶段，直接开始接受阶段： Chubby的Multi-Paxos实现 Chubby实现了闭源的Multi-Paxos，通过引入Leader节点。Leader是通过执行Basic Paxos投票产生的。 运行过程中会通过续租的方式延长租期，如果Leader故障，其他节点会选举出新的Leader。 所有的读和写操作也只能在Leader上进行：\n 写请求，Leader收到客户端的写请求，作为唯一的Proposer执行Basic Paxos将数据发给所有的节点来达成一致，半数以上的服务器接受了写请求之后，响应给客户端成功 读请求，很简单，Leader直接查询本地数据返回给客户端即可。  Chubby的ulti-Paxos实现的一些点：\n Leader本地的数据一定是最新的。 可以容忍$\\frac{n-1}{2}$个节点的故障  Raft Raft算法在Multi-Paxos的思想上进行了简化和限制，是最常用的一个共识算法，也是目前分布式系统的首选共识算法。包括Etcd、Consul等。\n本质上来说，Raft算法是通过一切以领导者为准的方式，实现一系列值的共识和各节点的日志一致。\n强烈推荐看一下这个可视化的Raft，可以加深理解：http://thesecretlivesofdata.com/raft/\nLeader选举 服务器节点的状态分为三种：Leader（领导者）、Follower（追随者）、Candidate（候选人），其中Leader有且只有一个。\nLeader：系统的核心角色，负责处理写请求、管理日志复制和不断与其他节点维持心跳，告知节点Leader存活，不要选举 Follower：普通群众，接受和处理来自Leader的消息，如果Leader心跳超时就主动站出来变成Candidate Candidate：候选人，向其他节点发送RequestVote的RPC消息，通知其他节点投票，一旦获得了多数投票就晋升为Leader\nRaft算法实现了随机超时时间，每个节点等待Leader的心跳超时时间随机。\n初始时没有Leader，都是Follower，所有节点听不到Leader心跳，超时时间最小的节点首先称为候选者。 它会增加自己的任期编号，给自己先投一票，然后发送RPC请求其他节点投票。 其他节点收到RPC投票消息之后，如果还没有称为候选者，也还没投票的话，就会去投一票，同时增加自己的任期编号。 如果在选举超时时间内获得了大多数的选票，就晋升为Leader。\n 关于RPC Raft算法总共有两类RPC，一个是请求投票RequestVote，一个是日志复制AppendEntries\n  关于timeout 每个节点的等待时间有两种： ① 一个是election timeout，也就是从上一次Leader心跳开始算，如果过了这个timeout还没听到心跳，就自己称为Candidate，这个timeout一般是150-300ms ② heartbeat timeout\n  关于任期 任期由单调递增的数字（任期编号）标识 何时加1？Follow发现Leader心跳超时，将自己任期+1，并发RPC 何时更新？ ① 跟随者接受到包含任期的RPC请求后，发现任期比自己的大，就更新自己的任期为更大的任期。 ② Leader或者Candidate发现自己的任期编号比其他节点小，会立即降为Follower\n 如果一个节点收到一个包含任期编号比自己小的RPC请求，会直接无视。 任期编号相同时，日志完整性高的Follow会拒绝投票给日志完整性低的Candidate\n 做法就是RequestVote RPC也会包含Candidate自己最后一个log entry的index和term，如果收到RequestRPC的节点发现这个Candidate最后一个log的term小于自己的term，或者term相等的时候index小于自己的index，那么就不会投票给它。这一策略保证了Leader一定拥有最完整的log entries\n 可能会出现多个Candidate同时发起投票请求，这样的话瓜分选票会导致无法选出半数以上的票，不过Raft通过随机超时时间解决了这一问题，把超时时间都进行了分散。这里的超时时间有两种，一个是Follower和Leader维持的心跳超时，一个是等待选举超时的时间间隔。\n当然还有可能出现的极限情况，比如说刚好两个Candidate各拿到了一半的票，那么陷入阻塞，此时这两个Candidate还会有随机timeout，如果时间过了就重新发送RequestVote\n几个注意的点：\n 只有日志最完整的节点才能当Leader，Raft中，日志必须是连续的\n 日志复制 日志项包含指令、索引值、任期编号等。\n第一阶段，Leader通过日志复制AppendEntries，将日志项复制到集群的其他节点上，如果收到了大多数的“复制成功”消息，就把提交这条日志，并返回成功给客户端，否则会返回错误给客户端。\n 一开始只是保存日志到本地，比如客户端提交一条SET x = 5，Leader会先把这条写在日志里，不会修改x的值，等到多数节点返回成功之后才会执行这条指令，把x设为5。\n AppendEntries RPC在每个heartbeat都会发送\nLeader不需要发送消息来告知其他节点提交日志项，Leader的日志复制RPC和心跳包含了当前最大的将被提交的日志项。从而将二阶段简化为一阶段。\n具体的过程为：  客户端提交一条写请求 Leader将其存在本地日志上，然后给各个Followers发送日志复制AppendEntries RPC， 如果有多数的Follower返回成功，Leader就将日志进行提交 Leader将执行的结果返回给客户端 之后如果Follower收到新的日志复制RPC或心跳，发现自己有日志项没提交，就进行提交  日志一致性的保证： Leader的日志必然是完整的，以Leader的日志为准来协调各个节点的日志。\n首先通过AppendEntries RPC的一致性检查来找到自己与Follower相同日志项的最大索引值，之前的日志Follower和Leader一致，之后的就不一致了，然后Leader强制Follower覆盖不一致日志。\n引入两个变量： PrevLogEntry：当前要复制的日志项的前一项的索引值，下面例子中为7 PrevLogTerm：当前要复制的日志项的前一项的任期编号，下面例子中为4\n Leader发送AppendEntries RPC，包含当前任期编号4、PrevLogEntry=7、PrevLogTerm=4， Follower发现自己的索引中没有这一条，返回Failure Leader递减要复制的日志项的索引，发送PrevLogEntry=6、PrevLogTerm=3 Follower能在本地日志找到这一项，返回Success Leader知道了自己与该Follower的相同日志的最大索引，复制并更新覆盖索引值之后的日志项。   为什么要上一条？因为这一条的话刚写，必然不一样，如果Follower一直和Leader一致，Follower是有Leader的上一条的，但是必然没有Leader新的一条，所以Leader如果从最新的一条发RPC，每一个节点都必然返回Failure，然后递减，非常浪费RPC。 由于大部分节点是能同步日志的，所以第一次都会返回Success，然后Leader把新的一条复制过去即可，对于第一次Failure的个别节点，才会递减找到相同的最大索引值。\n 成员变更 在成员进行变更的时候，如何避免出现大于一个的Leader？ 比如出现了分区，节点被分为了多个簇，簇与簇之间无法沟通，那么每个簇内都会有一个Leader。\n如果只是为了解决不出现多个Leader的情况，最暴力的方式就是节点全部关闭然后再重新启动，这样投票只会有一个Leader，但是这段时间系统会瘫痪，明显不合理。 最常用的方法是单节点变更，也就是每次只变更一个节点。\n比如当前集群配置为[A, B, C]，现在往里面加入[D, E]，一个一个加，先加D进去： 首先Leader向D同步所有数据，然后Leader更新自己的配置为[A,B,C,D]，将包含新配置的日志项提交到本地状态机，完成单节点变更，之后E加入也一样。 通过单节点变更，可以保证系统只有一个Leader。\n可以看一下Raft作者讲的： https://www.youtube.com/watch?v=vYp4LYbnnW8\n总结 以这张图进行总结：\n对于上述共识算法进行比较：\n Backup，简单备份  比如mac的time machine。 首先对于Consistency一致性，是无法保证的，一旦改变当前的文件，备份的旧版本和目前的会不一致。 Transaction事务也只能是weak。 Latency（时延）低、Throuput（吞吐量）高，因为读写的时候不需要运行协议，直接读取即可。 Data loss，如果没备份完，系统失效了，那最新的资料会遗失。 Failover 故障恢复，系统恢复的这段时间系统是不能工作的。\nMaster/Salve 主从模式  读写请求都在master上进行，master将更新 的数据写到slave上。类似Dropbox。\nConsistency：可以实现最终一致性 Transactions：Master支持完整事务 Latency、Throuput：读写的时候都直接在master上完成，所以低时延、高吞吐。 Data loss：可能造成数据丢失 Failover：恢复的时候slave还是可以提供read\nMaster/Master  Leaderless模式，每一个节点都可以接受读写请求。比如DynamoDB。\nConsistency：可以实现最终一致性 Transactions：只能本地支持 Latency、Throuput：低时延、高吞吐。 Data loss：可能造成数据丢失 Failover：仍能正常运作\n2PC  二阶段模式\nConsistency：强一致性 Transactions：支持完整事务 Latency、Throuput：因为每次都需要两阶段，比较差 Data loss：只要写入后资料达成一致就不会丢失 Failover：仍能正常运作\nPaxos \u0026 Raft  虽然表中没有raft，实际上raft和paxos也差不多。 Paxos可以认为是优化2PC之后的最优解。\nConsistency：强一致性 Transactions：支持完整事务 Latency、Throuput：需要半数达成一致，相对差一点 Dataloss：只要写入后资料达成一致就不会丢失 Failover：只要有半数的节点存活就可以正常运行。\n参考：\n https://ithelp.ithome.com.tw/users/20121042/ironman/2792 https://time.geekbang.org/column/intro/100046101  ","description":"","tags":["分布式","精选"],"title":"分布式系统（二）：共识算法","uri":"/posts/distribution-system-2/"},{"categories":["理论"],"content":"ACID、BASE、2PC/3PC ACID 在讲ACID之前，先讲本地事务，事务最早在数据库等课程中就接触过，简单来说，事务提供一种“要么什么也不做，要么全做完”的机制。\nACID特性是数据库事务的基本特征，包括：\n Atomicity 原子性 Consistency 一致性 Isolation 隔离性 Durability 持久性  合称就是ACID（在英语中正好是酸的意思，之后的BASE碱也与之对应）\n然而分布式事务和本地事务不同，假设有一个操作需要多个机器上执行，要么都执行，要么都不执行。 要保持分布式事务的ACID，方法有二阶段提交协议和TCC。\n2PC二阶段提交协议 一个事务跨越多个节点，成为分布式事务，为了保持ACID，需要引入一个协调者的角色来统一掌控所有节点的结果。\n整个过程被分为两个阶段：\n 准备阶段（投票）  协调者给每个参与者发送Prepare信息，每个参与者有两种选择： ①返回失败 ②本地执行事务返回成功，但不提交。询问之后的所有事务操作都记log，以便之后的恢复。\n提交阶段（执行）  如果协调者收到了失败或者超时，就直接给每个参与者发送回滚消息，否则就发送提交（commit）消息。 参与者如果收到提交消息，就提交事务，并释放资源和锁。如果收到回滚消息，就回滚事务，并释放资源和锁。\n二阶段提交协议的缺点：\n 同步阻塞，从投票开始到提交完成的这段时间，所用的资源被锁死 单点故障，如果协调者故障了，就会一直阻塞 数据不一致，第二阶段发送commit时可能部分节点因为故障收不到，导致只有一部分执行了commit。 等  之后有提出三阶段协议3PC，对二阶段协议进行了改进，然而由于增加了通信成本，实际用的并不多，就不细讲。\nTCC（Try-Confirm-Cancel） TCC是一个业务层面的协议，需要在业务代码中编写，包含了预留、确认或撤销三个阶段。 核心思想是针对每个操作都要注册一个对其对应的确认操作和补偿操作。 首先是try阶段，先通知各个节点的将要进行的操作。 如果try阶段的回复都是ok，就执行确认操作，通知各个节点要执行操作；如果try阶段有错误或者超时，就执行撤销操作，\n可以说ACID是CAP一致性的边界，也就是最强的一致性。\nBASE BASE则是追求可用性，是CAP中AP的拓展。\nBASE的核心是基本可用（Basically Available）和最终一致性（Eventually Consistent）\n比如遇到峰值，可以用四板斧解决：\n 流量削峰，将访问请求错开，比如多个秒杀商品放在不同的时间开始 延迟请求，比如买火车票抢票的时候等一段时间系统才处理 体验降级，比如先用小图片代替原始图片 过载保护，请求放入队列中排队处理，超时了就直接拒绝，队列满了之后就清除一定的请求。  目的是在基本可用性上保持妥协，谁也不想牺牲这些服务，但是为了可用性必须这样。\n最终一致性是指所有数据副本在经过一段时间的同步之后，最终能保持一致性。 显示生活中，除了金融等对一致性要求极高的领域，它们会使用强一致性。绝大部分互联网系统都采用最终一致性。\n实现最终一致性的方式用的多的有以下几种：\n 读时修复，查询数据的时候如果检测到不同的数据，系统自动修复 写时修复，写失败的时候先将数据缓存下来，之后定时重传 异步修复，最常用，通过定时对账来检测副本数据的一致性并修复  然而异步修复和读时修复的开销比较大，需要进行一致性对比，而写时修复的开销低。\n如果要设计分布式数据库的一致性的时候，可以采用自定义写一致级别（All、Quorum、One、All）来让用户自主选择业务所适合的一致性级别\nBASE通过牺牲强一致性来获得高可用性。\nCAP 分布式系统的最大难点之一就是维护各个节点之间的数据状态一致性。 需要通过数据库或者分布式缓存来维护数据的一致性。\nCAP是三个缩写的组合：\n C（Consistency）：数据一致性，分布式系统中，同一份数据可能存在于多个实例中，其中一份的修改必须同步到所有它的备份中。也就是说每一次必然能读到最新写入的数据，或者返回错误。 A（Availability）：服务可用性，服务在接收到客户端请求时必须要给出响应。在高并发和部分结点宕机的情况下依然可以响应。也就是每一次必然会返回结果，但是不保证是最新的正确的。 P（Partition tolerance）：分区容忍性，由于网络的不可靠性，位于不同网络分区的结点可能会通信失败，如果能容忍这种情况，那么就满足分区容忍性。也就是说出现问题能够容忍。  一个分布式系统不可能同时满足这三个基本需求，最多只能满足两项。\n 满足CA  也就是必然一致而且能够返回正确结果，这样是不存在的，其实就是单Server，不叫分布式系统。\n满足CP  牺牲A，只要系统中有一个Server没更新完，就返回错误，否则就返回正确的最新的值。\n满足AP  牺牲C，也就是只要Server接收到请求就返回目前的值，但是不能保证一定是最新的正确的值。\n分布式系统必须满足分区容忍性，也就是只能从A和P中进行取舍，数据一致性和服务可用性只能满足一个。当然实际情况不可能只顾一个而完全放弃另一个，而是在主要关心一个的前提下尽量满足另一个。\n比较成熟的服务注册与发现有以下几个：Consul、Etcd、Zookeeper、Eureka\n其中Consul、Etcd、Zookeeper满足了CP，而Eureka满足了AP。\n一致化模型Consistency Model 对Consistency的不同程度的要求也衍生出了多种不同的等级模型。根据不同的情况采取不同的模型。\n假设有一场球赛，记分员负责将分数写入主Server，然后会将操作复制到各个replica server，读取分数的话可能是任意一个server。\nk=0,1分别代表主队和客队，如果主队得了一分，记分员操作是：\n v = get(k) set(k, v+1)  假设目前比分是2:5\nStrong Consistency 对于任何一个人，读到的一定是最新的\nEventual Consistency 只把结果给其他Server，只能保证最后的时刻会更新到正确的最终值，但是之前读到任何小于结果的得分都有可能，甚至是完全没出现过的得分，比如2:0\nConsistent Prefix 连同操作一起给其他Server，从而保证读到的一定是比赛中的某个比分，历史发生过。\nBounded Staleness 保证读到的一定是t以内的结果。Bounded=0则为Strong Consistency。Bounded=无穷则为Eventual Consistency。\nMonotonic Reads 可能返回任何结果，但是接下来会持续从同一个replica server中读取，保证每一次都至少会比之前的值新。\nRead My Writes 如果某个client对Server进行了set操作，那么之后的get必然是set的值。\n不同的角色，要求的系统模型不一样。 记分员：只有他会写入系统，用Read My Writes 裁判：只能Strong Consistency 报分员：保证是历史正确比分，然后每一次至少比上次新，Consistent Prefix+Monotonic Reads 记者：Bounded Staleness就可以，多等点时间 观众：无所谓，Eventual Consistency都行\n银行的系统必然是Strong Consistency，只能最新。而DNS只要是Eventual Consistency就可以，因为需要快速返回结果，不是最新的也可以接受。\nQuorum System Quorum System随着Amazon与2007年发表的Dynamo: Amazon’s Highly Available Key-value Store论文而提出，这篇论文是NoSQL的代表之作。DynamoDB是一个NoSQL数据库，支持键值和文档数据结构，具有Strongly Consistent和Eventually Consistent。\n之前都是往一个Leader Server里写入，然后复制到replica server里，而我们更需要的是写入的时候任何一个Server都可以，读取的时候也是任何一个Server都可以。 也就是Leaderless Replication\n但是这样做的问题在于，如果两个写入操作的时间比较靠近，很可能出现对于不同的服务器而言，指令到达的时刻顺序不一致，从而错误。\n一种方法是每次写入都加锁，也就是去抢每个replicas server的锁，直到都写完了才释放所有的锁，让下一个写入进入。但是这样的话过于严格，效率低下。\n把条件放松一些。 对于写入操作，当一个client取得w个replicas的Lock才被允许写入。 取得R个replicas的Lock才被允许读。 写入时搭配timestamp。\n只要W+W\u003eN就可以防止同时写的发生，保证不会出现最新的值不明确的情况。这个不解释。\n只要W+R\u003eN就可以防止同时读写的发生，保证不会出现读取的值不是最新值的情况。配合timestamp之后，根据抽屉理论，读的时候至少会读到一台最新的server，从而根据timestamp可以找出它。\n通过Quorum System，可以不必设置primary server、replica server的形式，直接对任一server进行读写，仍然能保证Strong Consistency。\n这样的话，通过使用DynamoDB，Amazon会在世界各个地方的数据中心存放你的数据，进行备份，也能通过local replica进行加速。\nRead-Repair和Anti-Entropy 如果说有几个节点瘫痪了，导致每个都无法拿到超过一半的锁。\nRead-Repair就是在读取的时候不仅通过timestamp拿到最新的结果，还顺便将最新的结果写回其他的server里去。这种适用于频繁读取的情况。\n另一个方法是Anti-Entropy，也就是单独创建一个process，通过检查replica的版本并将所有server都同步成最新的。适用于读取不频繁的情况。\nHinted Handoff 故障的server恢复之后，系统会写回这个server，这种做法叫Hinted Handoff。 写失败的请求会缓存到本地硬盘上，并周期性的尝试重传。\nQuorum NWR 对于AP系统，可以保证最终一致性但是无法保证强一致性。如果想满足强一致性，可以借助Quorum NWR。\nQuorum NWR可以根据业务的特点，调整一致性级别。\n三个要素：N、W、R\nN：复制因子，也就是一个集群中，数据有多少个副本，当然不同的数据可能有不同的副本数 W：写一致性级别，成功完成W个副本更新，才完成写操作 R：读一致性级别，读一个数据对象需要读R个副本\nW + R \u003e N：不会出现并行读写，一定能读到最新值 W + W \u003e N：不会出现并行写 W + W \u003c= N：可能出现不一致 W + R \u003c= N：可能会读不到最新的值 R + R \u003e N：\n分布式系统的时间 通常会采用W+W\u003c=N来尽量保证Availablity，这种情况下如何规避并行写导致的不一致呢。\n一个方式就是要了解两个指令在发出时的先后顺序，而不是到达时的顺序，从而保证一致性。看起来通过发出信号时就附加timestamp可以解决问题，看起来每台机器的时间是一样的，然而实际上并不一定。\n由于每台机器自身的时间并不一定准确，甚至可能会出现接受到信息的timestamp比机器当前时间还要晚的情况（收到来自“未来”的消息），这样就很离谱，明显不合理。\nLamport Logical Clock 在消息里夹带一个timestamp，但是在传递的时候，每个结点接受到timestamp后，会比较自身时间与timestamp的大小，然后选择最大的那个置为新的timestamp，从而保证一定递增。收到的消息的timestamp比本身时间还大的话，就将自己的时间改为timestamp的时间。\n 每个参与者最开始都保存一个timestamp=0 如果在本地发生，timestamp+1 如果传递这个消息，timestamp+1，然后传递时附带该timestamp 如果接受这个消息，timestamp = Max(本地Clock, 消息timestamp) + 1  （Lamport发明了Latex）\nVector Clock Lamport timestamp会显示两个先后的事件有因果关系，但是实际逻辑上并不一定，可能只是同时平行发生。\n对于N个Node的系统，Vector Clock让每个Node都存储一个长度为N的timestamp vector\n对于$Node_i$而言，存储$Vector_i = {t_0, t_1, ..., t_n}$\n 初始化每个Node的vector中的每个元素都为0 $Node_i$发生一个事件，$V_i[t_i]+1$ $Node_i$发生一个发送事件，$V_i[t_i]+1$，并夹带这个vector $Node_j$发生一个接受事件，$V_j[t_i] = V_i[t_i]$、$V_j[t_j] = Max(V_j[t_j],V_i[t_i]) + 1$  参考：https://ithelp.ithome.com.tw/users/20121042/ironman/2792\n","description":"","tags":["分布式"],"title":"分布式系统（一）：CAP及基础理论","uri":"/posts/distribution-system-1/"},{"categories":["算法"],"content":"介绍 为什么需要Bigtable？ 需要一个集群支持海量的随机读写，需要支持到每秒百万级别的随机读写。在Bigtable没出之前，使用MySQL集群可以解决一些问题，然而一方面会放弃关系型数据库的很多特征，比如外键约束、跨行跨表的事务等。一方面在扩容的时候不得不翻倍扩容，非常浪费。缩减服务器也非常麻烦。另外，在每次故障恢复的时候也需要人工介入。\n希望的伸缩性是可以随机增加或者去掉人任何数量的服务器，并且进行这些操作时不会使服务暂停。\nBigtable建立在GFS的架构之上，是一个管理结构化数据的分布式存储系统，可以拓展到非常大的规模，比如跨越数千服务器的PB级别的数据。 Google已经将其用在了很多内部产品中，Bigtable为其提供了一套高性能的可灵活拓展的解决方案。\n在很多方面，Bigtable像是数据库，但相比于以往的系统，Bigtable提供了不一样的接口。它不支持完整的关系型数据模型。可以使用任意字符的行列名对数据进行索引，Bigtable将数据都视为未解释的字符串。\n当然Bigtable也有缺点，一个是放弃了关系模型，不支持SQL；一个是放弃了跨行的事务，只支持单行的事务模型。\nBigtable的解决方法是：\n 将存储层搭建在GFS上，通过单Master调度多Tablets的形式，使得集群容易维护，伸缩性好 通过MenTable+SSTable的底层文件格式，解决高速随机读写的问题 通过Chubby分布式锁解决一致性的问题  数据模型 Bigtable是一个稀疏的、分布式的永久存储的多维排序map，这个map通过row key、column key和timestamp进行索引，每个值都是一个未解释的字符串。\n(row: string, column: string, time: int64) -\u003e string\n下图是一个存储网页的table\n row是url的倒转，比如www.google.com会存为com.google.www，这样的目的是前面的www大家都一样，而且子域名就会主域名靠一起。 有多列，其中contents:列存储网页html内容。anchor:列存储指向这个页面的anchor文字，比如cnnsi.com和my.look.ca有指向www.cnn.com的anchor，就如下图所示存储。  Rows 行key是表的主键，可以是任意字符串，最大为64kb，在单行的读写都是原子的。 由于读写总是通过行键，这样的数据库也叫做KV数据库。 Bigtable按行key对数据进行排序，行范围动态分区，每个行的范围被称为tablet，是分布式和负载均衡的单位。\nColumn Families 列族 每一行的数据需要指定列族，每个列族下不需要指定列，每个数据都可以有自己的列，每一行的列可以不一样。这也就是为什么说Bigtable是稀疏的表：\n列key被分组到了一个集合里，被称为column families，每个column families里的应当是相同类型。必须先创建column families，才能使用列key存储数据。 列key通过family:qualifier命名。比如存储web的表可以用language当做family，另一种是可以用anchor来当做family，每个列key是一个anchor，qualifier是指向该url的网址，内容是链接文本。 访问控制和硬盘内存的记录都是在列family层级下进行的。 比如Bigtable的开源实现HBase，每一个列族的数据存在同一个HFile文件下。\nTimestamp Bigtable的每个单元格可以包含相同数据的多个版本，不同的版本通过时间戳进行索引。Bigtable的时间戳是64位的整数。不同版本以递减的形式存储，以便可以首先读取最新版本。\n为了防止变得过于繁重，可以指定个数或过期时间，之前的版本被gc。\nAPI Bigtable的API包括创建、删除表和列族，以及修改簇、表、列族元数据等。\n1 2 3 4 5 6 7 8 9  // Open the table Table *T = OpenOrDie(\"/bigtable/web/webtable\"); // Write a new anchor and delete an old anchor RowMutation r1(T, \"com.cnn.www\"); r1.Set(\"anchor:www.c-span.org\", \"CNN\"); r1.Delete(\"anchor:www.abc.com\"); Operation op; Apply(\u0026op, \u0026r1);   1 2 3 4 5 6 7 8 9 10  Scanner scanner(T); ScanStream *stream; stream = scanner.FetchColumnFamily(\"anchor\"); stream-\u003eSetReturnAllVersions(); scanner.Lookup(\"com.cnn.www\"); for (; !stream-\u003eDone(); stream-\u003eNext()) { printf(\"%s %s %lld %s\\n\", scanner.RowName(), stream-\u003eColumnName(), stream-\u003eMicroTimestamp(), stream-\u003eValue()); }   构建块 Bigtable使用GFS存储日志和数据文件，SSTable用于存储Bigtable数据，每个SSTable包含一个块序列（每个块64kb），并且SSTable可以被完全的映射到内存中，不需要接触磁盘就可以执行查找和扫描。\nBigtable依赖于分布式锁Chubby，Chubby包含了5个副本，其中一个被选为master并提供request服务。我后面会专门再讲一下Chubby。\nBigtable通过Chubby完成以下任务：\n 确保每个时刻只有一个master 存储Bigtable数据的引导位置 存储Bigtable每个表的列族信息 存储访问控制列表ACL  如果Chubby不可用，那么Bigtable也将不可用\n实现 Bigtable包含三个主要组件：\n 链接到每个客户端的库 一个master服务器 多个tablet服务器  tablet可以动态的增加删除。\nmaster的职责：\n 负责将tablet分配给tablet服务器 检测tablet的添加和过期 平衡Tablet server之间的负载 对GFS的文件进行gc 管理Table和列族的Schema变更  每个tablet服务器存储一组tablet（通常是10-1000个），\nBigtable和Tablet Server都不进行数据的存储只负责在线业务，存储工作通过SSTable的数据格式写到GFS上。\nTablet位置 通过B+树存储tablet的位置\n定义了一张特殊的表Root tablet专门存放元数据，这个分区不会分裂，存的是元数据里其他Tablets的位置。\n第一级存储在Chubby的文件，包含root tablet的位置，root包含metadata tablets，包含了其他所有tablet的位置。tablet不做分割，确保不超过三层。\n举个例子，客户端查询ECOMMERCE_ORDERS业务表行键是A20210101RST的某个记录，客户端查询的具体操作：\n也就是说在具体查找数据之前需要三次网络请求来获得数据的具体位置。一般前几次的查询也会缓存起来，以减少请求次数。\n三层结构可以让Bigtable拓展到足够大，tablet大小限制为128MB，每条记录大约1KB，可以存$2^{34}$个Tablet，也就是160亿个Tablet。\n客户端不需要经过master，让设计更加高可用\n动态分区 Bigtable采用动态区间分区，通过自动去split的方式动态分区。 好比是往箱子里放书，按照书名的字母顺序，一旦箱子装满，就中间一分为二，将下面一半放到一个新的空箱子里去。 如果两个相邻的箱子都很空，就可以将其合并。 SSTable底层结构 Bigtable的写入数据的过程：\n tablet server先做数据验证，以及权限验证 如果合法，就以追加写的形式顺序写到GFS 写入成功后还会写到一张内存表MenTable中 写入的数据快要超过阈值时，会将内存的MemTable冻结，创建一个新的MemTable，被冻结的MemTable会被转换为SSTable写入到GFS，然后从内存中释放掉。  Major Compaction机制，对SSTable进行合并，把数据压实在一起，比如只留下时间戳最近的三个版本的数据。 读取数据的时候，读取的是MemTable和SSTable的合并在一起的视图。 也就是说并没有直接的修改和删除操作，一旦写入就是不可变的，写入的是数据的一个新版本，后台会定时gc，通过合并SSTable来清楚过期和被删除的数据。\n总结 Bigtable包括四个组件：\n 负责存储数据的GFS 负责作为分布式锁和目录服务的Chubby 复杂提供在线服务的Tablet Server 复杂调度Tablet和调整负载的Master  ","description":"","tags":["Bigtable","分布式","大数据"],"title":"Google三驾马车（三）—— Bigtable","uri":"/posts/bigtable/"},{"categories":["算法"],"content":"介绍 MapReduce是一个用于处理和生成大型数据集的编程模型和相关实现，它是一个分布式模型，通过一个Map函数将k/v对生存一组中间态的k/v对，然后通过一个reduce函数将所有的中间态k/v对进行聚合。 MapReduce运行在一个大型的商用机器集群上，比如可以在数千台机器上处理大量TB级别的数据。 实际上Google早已将其用于实际的任务，每天有超过1000个MapReduce任务在谷歌的集群上运行。\n面对百亿级别的爬虫数据、日志文件等，常规方法不可能做到时效性，只能采用分布式系统进行并行计算。\n编程模型 输入是一系列k/v对的set，输出也是一系列k/v对的set\n用户需要编写Map和Reduce这两个函数，其中Map函数通过输入pair来产生中间过程的k/v对 接下来会将Key为I的中间值传递给对应处理Key I的Reduce函数 Reduce函数接受一系列的Key为I的值，然后merge在一起，每次只有0或1 具体的见下面的例子。\n例子 输入文件首先分块， 需要一个Map函数，每个输入文件输入Map进行处理，每个都是并行的，产生对应的输出，输出是一个list形式的Key/Value的键值对。\n假设我们的功能是读取字符出现的次数。 假设输入为\"abbac\"，被拆成了三个文件，总共也就这三个Key。分别是\"ab\",\"b\",\"ac\"，并行输入进Map，三个输出分别为： (a,1), (b,1) (b,1) (a,1), (c,1)\n然后进行reduce操作，对于每个Key，会传入reduce函数进行汇总，去统计每个Key的出现个数。这也是并行的。\n那么经过reduce操作之后，输出为： (a, 2) (b, 2) (c, 1)\n完整的Job由一系列的MapTask和一系列的reduceTask组成。\n下面来说说对于统计字母的功能下，Map和Reduce这两个函数的结构：\n1 2 3 4 5 6 7  // Map函数，k指明文件，v是文件内容 Map(k ,v){ split v to words; for each word w{ emit(w, \"1\") } }   1 2 3 4  // reduce函数，k是这个字母，v是包含这个字母的map数量 reduce(k, v) { emit(len(v)) }   MapReduc和GFS都运行在一起，在并行进Map的时候，实际上避免了网络传输，中控通过某些方式能够知道该文件存在哪台主机里，然后在该主机调用Map本地操作，从而减少带宽传输限制。后面reduce只能通过网络。\n最开始是按行存储，然后按列存储，这个过程叫Shuffle，从Map服务器到Reduce服务器，这一过程很消耗网络。\n类型 map函数 输入是(k1, v1)，输出是list(k2, v2) reduce函数 输入是(k2, list(v2))，输出是list(v2)\n以统计词频为例，这里map的输入是(filename, fileContent)，输出是对于每一个单词为key的k/v列表，比如[\"Apple\": 1, \"Banana\": 1,...]。 这里reduce的输入就是一个单独单词的一系列值，比如\"Apple\", [1, 1, 1,...]，然后输出是该单词的词频。\n更多的例子 除此之外还有很多适用于MapReduce的很好的例子。\n实现 分割输入数据，分成M个子集，被调用分布到多台机器上并行处理。之后分割中间key形成R个片（比如通过hash(key) mod R），reduce调用分布到各个机器上。\n具体步骤：\n 分割输入文件为M个片，每个片的大小约16-64M 一个master，和多个worker，有M个map任务和R个reduce任务将被分配，管理者的一个任务是分配map或者reduce任务给一个空闲的worker 被分配了map任务的worker需要做的是读取输入片的内容，分析出k/v对，传递给用户自定义的map函数，产生的中间k/v对缓存在内存中。 缓存在内存中的k/v对通过分割函数写入R个区域，本地的缓存对的位置传送给master，然后master把这些位置传送给reduce worker。 reduce worker通过远程调用来从map worker的磁盘上读取缓存的内容，reduce worker通过排序使得具有相同key的内容聚集在一起。如果中间数据比内存还大，就需要外排序。 reduce worker迭代排过序的中间数据，对于每一个唯一的key，把key和相关的value传递给reduce函数，reduce函数的输出被添加到最终的输出文件中 所有的map和reduce都完成之后，管理者唤醒用户程序，用户程序的MapReduce调用返回到用户代码  Master的数据结构 master首先会存储每个map任务和reduce任务的状态（空闲、进行中、完成）以及工作机器的标识。 master还会存储由map产生的中间文件的区域和大小，然后传给reduce的worker\n容错  worker故障  master会定期ping每个worker，如果一段时间内没有收到响应，master就会将该结点标记为failed。正在进行的map或者reduce更是会重设状态，被调给其他worker。 然而，这些worker已经完成的map任务也会重新设置为idle状态，将会调度给其他的worker，这是因为它们的输出会存在故障机器的本地磁盘上，不过已经完成的reduce任务不需要重新运行，因为它们会存在全局文件系统上。\n如果一个map任务在A worker上执行，然后A挂了，被调度给了B worker。所有的在做reduce的worker都会被通知到这个，然后读取对应的中间数据会从B读取。\nMapReduce对大规模的worker故障有弹性。\nMaster故障  master会定期将内部的数据结构写到checkpoints里，如果master挂了，可以很容易的从最后一个checkpoints开启一个新的副本。\n失败时的Semantics  即使有故障，也能得到和没故障发生的情况下一样的输出。\n依赖于map和reduce任务提交的原子性\n本地 带宽是一种相对稀缺的资源，通过GFS存储在cluster的本地磁盘上。大部分输入数据会在本地读取，不消耗网络带宽。\n任务粒度 map任务被分为了M个片，reduce任务被分为了R个片，M和R实际上会远高于实际的worker机器，\nmaster将做$O(M+R)$的任务调度，以及保存$O(M*N)$个状态。\n备份任务 有时候可能会出现某个任务运行过久导致严重影响整体性能，比如某个worker机器的磁盘坏了导致非常慢的运行，它依然响应服务器的心跳不能认为是failed，但是运行就是非常慢。 MapReduce有一个备份任务的机制，就是当MapReduce即将完成的时候，也就是大多数任务都做完了，那么就会去备份还没完成的任务，只要原始任务或者备份任务的其中一个做完了就可以。\n改良拓展、性能表现与实验 上述已经是一个基本的MapReduce的任务了，一些改进拓展、性能表现与实验就不详细说明了，日后可以研究。\n","description":"","tags":["MapReduce","分布式","大数据"],"title":"Google三驾马车（二）—— MapReduce","uri":"/posts/mapreduce/"},{"categories":["算法"],"content":"介绍 GFS，即Google File System，谷歌文件系统。 它是一种能够用于大型密集型数据的可拓展的分布式文件系统。（大型存储系统），它对于廉价硬件提供了容错机制；对于大量客户的情况能有高表现。\nGFS的设计是由实际的应用程序负载和技术环境驱动的，与传统的文件系统的一些假设不一样。\n设计 一些假设  软硬件故障是常态而不是例外 文件是巨大的（多GB级也是普遍的） 负载包括大的流式读取和小的随机读取 负载还包括大的顺序的append写入 大多数文件是通过append而不是overwrite来改变的，一旦写入，就只能读取，而且是顺序读 放宽了一致性，从而极大简化了文件系统 引入了原子的追加写，可以并发的追加 高的持续带宽比低延迟更重要  一般用append，GFS对其一致性有保证，最好不用write\n接口 接口方面支持通用的create、delte、open、close、read、write。并且还有snapshot和append操作。 snapshot以低成本创建文件或者目录的副本，append允许多个客户端并发的追加同一个文件，保证原子性。 对于实现多路合并以及生产者-消费者模型很有用。\n架构 一个GFS集群由一个master结点和多个chunk sever构成，并被多个客户端访问。它们通常都是普通的Linux机器。\nGFS把文件切割为若干固定长度的Chunk块并存储，每个块的大小是64MB，在创建块时，对于每一个Chunk，master还会为其分配一个64位的全局唯一的Handle句柄。为了保证Chunk的可用性，每个块都会被复制到多个chunk server上，默认存储三个副本。\nmaster维护所有文件系统的元数据，包括namespace、访问控制信息、从文件到chunk的映射、以及块的当前位置。它还控制系统范围内的活动，比如chunk的租约管理、孤立chunk的gc、chunk server之间的chunk迁移。master定期与chunkserver维持心跳通信，给chunkserver指令以及接受它们的状态。客户端和chunkserver都不需要缓存文件数据，从而简化系统，唯一可能要缓存的可能就是客户机会缓存一下元数据。\nSingle Master 设立一个master可以极大的简化系统的设计，可以很方便地进行全局信息的管理。然而单一的master很容易成为系统的瓶颈，所以只能让其尽可能少的参与读写。客户端从来不从master中读写文件数据，而是向master询问它需要的文件在哪，然后访问这些chunkserver去进行文件交互。\n下面解释一下交互过程，首先客户端借助固定的块大小，将文件名和偏移量转换为块索引，然后向master发送包含文件名和块索引的请求，master返回一个chunk句柄和副本的位置。接下来客户端会向其中一个（往往是最近的）存储着该文件副本的chunkserver发送请求，之后对同一个chunkserver的交互不需要master的参与。事实上客户端通常一次会请求多个块。\nChunk Size 选择的是64MB，比典型文件系统的块大得多，相对于小的chunk size，更大的chunk size的优势在于：\n 减少客户端请求的chunk数量，减少客户端与master的交互需求。 大的chunk可以让客户端执行很多操作，通过较长时间与chunkserver的持续的tcp连接来减少网络开销 减少了chunk的个数，从而减少了存储在master的元数据的大小  当然，大的chunk size也有缺点：\n 可能会出现更多客户端访问一个chunk从而导致这个chunk成为hot spots。一般来说还好，不过如果某个可执行文件被写入了某个chunk，然后在数百台机器上同时启动，那个chunkserver就很容易超载。一个解决方法是将可执行文件复制更多份，并使批队列系统错开启动时间。还有一个解决方法是允许客户机从其他客户机读取数据。  Metadata 元数据包含文件和chunk的namespace、从文件到块的映射、以及每个chunk副本的位置，所有的元数据都存储在master的内存中。前面两个也通过日志的方式存储在本地磁盘中，实现持久性存储，顺带也复制在远程机器上备份。这个主要是保证即使master崩溃了也不会出现不一致。 至于chunk副本的位置，master并不会持久地存储，而是在master启动的时候对每个chunkserver进行轮询，或者在新的chunkserver加入集群时询问。\n 内存中的数据结构 元数据存储在内存中，所以访问起来很快。 master还会在后台周期性的扫描整个状态，用于实现gc、chunkserver故障时的重新复制、块迁移来平衡负载等。  可能会认为说元数据存在master内存中，整个系统的容量会受到master内存的限制，实际上chunk由于比较大，个数不会那么多，master也只需要存每个chunk的不到64字节的元数据，所以还好。\n  chunk位置 前面说了master通过启动时的轮询获得信息，并且还会保持一个心跳来监听各个chunkserver的状态。 由于集群很大，如果在master上持久化在本地存储chunk副本位置，之后变动会很多（改名、宕机、重启等），并且实际上chunkserver才是对chunk有着最终决定权，在master上维护一个一致性的视图是没有意义的。\n  操作日志 操作日志包含了元数据发生重大变化的历史记录，是GFS的核心。它是元数据的唯一持久性记录，也作为定义并发操作顺序的逻辑时间线。操作日志需要被可靠地存储。 如果系统崩了，master就会重新执行log来恢复GFS，所以log也不宜过大，以免启动时间过长。会先找到重载的checkpoint然后执行之后的日志记录。检查点是一种类似B树的紧凑形式，加快恢复速度。\n  一致性模型 GFS并不保持一个严格的一致性，而是保持一个相对宽松的一致性\n GFS保证的 命名空间是原子的，保证操作日志是全局的顺序正确的。  数据更改之后的文件区域的状态：\n文件数据更改之后，会定义一个region，其状态取决于变化的种类（write/append）、是否并行、成功还是失败。\n如果它是一致的，客户端会看到变化写入的内容。 如何区分已定义区域和未定义区域。\n在一次成功的顺序变化后，GFS会：\n 在chunk的所有副本上以相同的顺序应用这些变化 使用chunk版本号来检测副本  应用程序应当append而不是write。\n系统交互 描述客户端、master、chunkserver如何进行交互，完成数据更改、原子追加和快照。\n租约和数据更改顺序 数据更改（mutations）就是改变chunk的内容或者元数据的操作，比如write或append。数据更改在chunk副本上执行。 使用租约（leases）来维持副本之间的一致的变化顺序。master会将租约授权给其中一个副本，称之为该chunk的主服务器（primary）。主服务器会为这个chunk的所有更改进行顺序排序，其余的所有副本都遵守这个顺序进行更改。 租约机制的目的也是减少master的管理开销，租约的初始时间是60s，不过主要chunk发生了改变，primary就可以向master请求拓展，这些请求被承载在心跳信息上。\n下图是写操作的控制流与详细的步骤  客户端询问master哪个chunkserver持有当前chunk的租约，以及其他副本的位置。如果没有服务器有租约，master就选择一个副本服务器分给它租约 服务器返回primary和副本chunkserver的位置，客户端把它们存在缓存中，如果未来短期内再次访问就不需要请求master。除非primary不可达或者primary告知客户端它没有租约了。 客户端知道副本位置后，将数据push进所有的副本中，可以按照任何顺序。每个chunkserver将数据存储在一个内部的LRU缓存中 一旦所有的副本都确认接受到了数据，客户端就向primary发送写请求，标识了之前push的数据，primary会分配序列号给这些mutations，提供必要的序列化 primary将写请求转发给各个备用副本，每个备份副本按照序列号执行更改 备份副本回复primary表示已经完成了操作 primary响应客户端，任何遇到的错误也会报告  数据流 数据流和控制流解耦，为了充分利用每台机器的带宽，数据被线性的沿着chunkserver链进行推送，而不是分布在拓扑网络中，这样每台机器的带宽就可以被充分利用，每台机器将数据转发到网络拓扑中“最近的”没有接收到它的机器。（感觉像Prim算法）\n原子追加 GFS提供了原子追加（atomic record appends）操作。 传统的写操作需要提供数据和偏移量，如果出现并行的情况就很可能会出现来自多个客户端的碎片。 在GFS中，客户端只提供数据，GFS会选择偏移量并将其返回给客户端，类似于Unix的O_APPEND。\n大量使用record append，如果是传统的写操作，为了保持一致性就只能使用分布式锁，代价很昂贵。\nSnapshot快照 类似AFS，使用标准的copy-on-write技术实现快照。\nMaster操作 master的任务：\n 执行所有namespace相关的操作 管理系统的chunk副本以及与之相关的一些操作  namespace的管理和锁定 GFS没有传统文件系统的per-directory数据结构。也不支持alias。\ngc 文件被删除之后，不会立即回收资源，而是先重命名为包含删除时间戳的隐藏文件，如果隐藏文件存在超过三天，就删除它们。在此期间，这些文件可以被恢复。 内存元数据也会被删除，切断和所有chunk的联系，在和master的心跳中，chunkserver报告自己的chunks，master会返回不出现在namespace里的，chunkserver接受到后可以删掉这些chunk。\n容错性与诊断 高可用性 通过两种简单而有效的策略来保持整个系统的高可用性:快速恢复和复制\n  快速恢复 master和chunkserver都可以在几秒内启动\n  chunk复制 默认是复制3份\n  master复制\n  操作日志和检查点被复制到多台机器上\n","description":"","tags":["GFS","分布式","大数据"],"title":"Google三驾马车（一）—— Google File System","uri":"/posts/gfs/"},{"categories":["算法"],"content":"背景与简介 在生物医学、金融保险等领域，生存分析是一种很常见而且重要的方法。\n生存分析主要用在癌症等疾病的研究中，比如对某种抗癌药物做临床试验，筛选一部分癌症患者，分为两组，一组服用该试验药物，一组服用对照药物，服药后开始统计每个患者从服药一直到死亡的生存时间。\n生存分析可以抽象概述为，研究在不同条件下，特定事件发生与时间的关系是否存在差异。这些具体事件可以是死亡，也可以是痊愈、肿瘤转移、复发、出院、重新入院等任何可以明确识别的事件，而不同条件即为不同的分组依据，可以是年龄、性别、地域、某个基因表达量的高低、某个突变的携带与否等等。\n（后面均用\"死亡\"来代指这个特定事件\n概念与推导 生存时间T 把生存时间作为一个随机变量，用PDF（概率密度函数）和CDF（分布函数）来表达\n其中CDF为$F(t) = Pr(T \u003c t)$，也就是t之前死亡的概率\n生存概率 S(t)，Survival probability，研究对象从试验开始到某个特定时间点仍然存活的概率,$S(t) = pr(T \u003e t)$\n$S(t) = 1 - F(t)$\n之后的Kaplan-Meier模型主要关注S(t)\n风险概率 $h(t): \\text{Hazard function}$\n$$h(t) = \\lim_{\\epsilon \\to 0}\\frac{P(T \\in (t, t+\\epsilon] | T \\geqslant t)}{\\epsilon} = \\frac{f(t)}{S(t)}$$\n前一个等号的意义 很明显，表示的意义就是研究对象从试验开始到某个特定时间点t之前存活，但是在t时间点发生\"死亡\"的概率\n后面一个等号的推导过程\n$$ \\begin{array}{llr} h(t)\u0026 = \\lim_{\\Delta t \\to 0} \\frac{P(t \u003c T \\leqslant t + \\Delta t | T \u003e t)}{\\Delta t}\\newline \u0026 = \\lim_{\\Delta t \\to 0} \\frac{P(t \u003c T \\leqslant t + \\Delta t )}{\\Delta t S(t)} \u0026 \\scriptsize{S(t)的定义}\\newline \u0026 = \\lim_{\\Delta t \\to 0} \\frac{F(t + \\Delta t) - F(t)}{\\Delta t S(t)} \u0026 \\scriptsize{F(t)的定义}\\newline \u0026 = \\frac{f(t)}{S(t)}\u0026 \\scriptsize{f(t)是F(t)的微分} \\end{array} $$\n然后还可以进一步推导：\n$$ h(t) = \\frac{f(t)}{S(t)} = \\frac{f(t)}{1 - F(t)} = - \\frac{\\partial log[1 - F(t)]}{\\partial t} = - \\frac{\\partial log[S(t)]}{\\partial t} $$\n表示了$h(t)$和$S(t)$的关系\n$H(t): \\text{Comulative\\ Hazard\\ function}$ $$H(t) = \\int_0^t h(u) du$$\n进一步推导： $$H(t) = \\int_0^t h(u) du = - \\int_0^t \\frac{ \\partial log[S(u)]}{\\partial u} du = -log[S(t)]$$\n$$\\to S(t) = exp[-H(t)]$$\n之后的Cox比例风险模型主要关注H(t)\nHazard function理解 hazard function 本身不是概率，它描述的是一种在给定时间点的风险，$\\Delta t \\times h(t)$表示在$(t, t + \\Delta t]$的概率\nhazard function优势：\n 描述给定时间点的风险，这是我们需要的信息 可以很好的处理数据缺失的情况  举个例子 假设survival time服从指数分布$Exp(\\lambda)$，即$f(x) = \\lambda e ^{-\\lambda x}, x \u003e 0$\n也就是$f(t) = \\lambda e ^{-\\lambda x}$\n可以推出：\n$F(t) = 1 - e ^{-\\lambda x}$\n$S(t) = 1 - F(t) = e ^{-\\lambda x}$\n$h(t) = \\frac{f(t)}{S(t)} = \\lambda$\n$H(t) = \\lambda t$\n$E(T) = \\frac{1}{\\lambda} （指数分布的性质）= \\frac{1}{h(t)}$\n其他的分布同理 Gamma distribution Weibull distribution Log-normal distribution generized gamma distribution...\n删失数据 Censoring 生存分析中，很常见的一种特征就是删失数据\n指的是在临床试验中，出现一些数据丢失的情况，比如病人中途主动退出、无法联系到、结束时还未发生特定事件。保留了从一开始到丢失前进度的数据成为右删失，另一种称为左删失。（后面只讨论右删失）\nType I Censoring：观测时间确定 每一项数据增加一个表示：\n$$(U_i, \\delta_i) = {min (T_i, c), I(T_i \\leqslant c)}, i = 1, ... , n$$\n$$I(T_i \\leqslant C) = \\begin{cases} 1, \u0026 T_i \\leqslant C,\\ 0, \u0026 T_i \u003e C \\end{cases}$$\nc是实验时间，是一个常量 也就是说如果是$(c, 0)$，则代表被删失，如果是$(T_i, 1)$，则没有被删失\nType II Censoring：观测人数确定 比如观测n人，当死亡r人时停止试验 $T_{(1, n)}, T_{(2, n)}, ..., T_{(r, n)}$\nType III Censoring：随机Censoring 不用常量c而是用随机变量$C_i$\n$(U_i, \\delta_i) = {min (T_i, C_i), I(T_i \\leqslant C_i)}, i = 1, ... , n$\n只考虑右删失，我们只观察$(U_i, \\delta_i)$ 如果$(U_i, \\delta_i) = (u_i, 1)$，则说明$T_i = u_i, C_i \u003e u_i$ 如果$(U_i, \\delta_i) = (u_i, 0)$，则说明$T_i \\geqslant u_i, C_i = u_i$\n(推导见https://www.bilibili.com/video/BV1WE411P78Z?p=2)\nKaplan-Meier模型 与生存表、Cox并列的一种生存分析的方法，也叫乘积极限(product-limit estimator)\n$\\hat{S}(t)=\\prod_{i: t_{i} \\leq t}\\left(1-\\frac{d_{i}}{n_{i}}\\right), \\quad t \\geq 0$\n$d_i$是在$t_i$时刻死亡的人数，$n_i$是还在风险中的人数\n例子：\nLife table 生存表 举例：\n设时间点为$t_0, t_1, ... ,t_n$，那么在$t_i$时间点下的生存概率： $$S(t_i) = \\Pi_{j=0}^{i}(1-P(t_j死亡))$$\n也就是： $$S\\left(t_{i}\\right)=S\\left(t_{i-1}\\right)\\left(1-\\frac{d_{i}}{n_{i}}\\right)$$\n$n_i$表示$t_i$时的有效人数，$d_i$表示$t_i$时的死亡人数\n$t_i$处的生存率等于$t_{i-1}$时的生存率乘以（1-$t_i$时间点的死亡率）\nKaplan-Meier 生存曲线： 加号表示删失数据\n往往是多条线（因为是不同的组）\nCox比例风险回归模型 Cox Proportional-Hazards Model是由英国统计学家D.R.Cox于1972年提出的一种半参数回归模型（半参数值既包含参数模型，又包含非参数模型）\n参数模型：有限维度，有限个参数就可以表示模型分布，比如正态分布的均值和标准差 非参数模型：属于某个无限维的空间，无法用有限个参数来表示，比如决策树、随机森林\nCox建立回归的是前面提到的$h(x)$ Cox模型： $$h(t) = h_0(t) \\times exp({b_1x_1 + b_2x_2 + ... b_px_p})$$\n其中$h(t)$指的是不同时间的风险值（hazard），$x_i$指的是具有预测效应的变量，$b_i$指的是每个变量对应的效应值，$h_0(t)$是基准风险函数，根据不同的数据来使用不同的分布模型，是非参数模型\n建模时，首先确定需要研究的可能影响生存率的因素，也就是$x_i$，我们主要要做的就是找到合适的$h_0(t)$以及所有协变量的系数$b_p$，需要用到极大似然估计等方法求解参数。\n两个基本假设 对公式两边取对数进行变形：\n$$log(h(t)) = log(h_0(t)) + \\beta X$$\n 模型中各危险因素对危险率的影响不随时间改变，且与时间无关 对数危险率与各个危险因素呈线性相关  参数的极大似然估计 通过极大似然估计来求解参数，极大似然估计的思想是，让已经发生的事件出现的可能性最大。\n举个例子，有三个人$X_1, X_2, X_3$分别在三个时间点$t_1, t_2, t_3$死亡\n以$t=t_1$为例，此时我们的目标是$max\\ h(t_1, X_1)$和$min\\ h(t_1, X_2) + h(t_1, X_3)$，统一这两个的目标：\n$$max\\ \\frac{h(t_1, X_1)}{h(t_1, X_1) + h(t_1, X_2) + h(t_1, X_3)}$$\n（分母加一个分子不影响结果，但是可以让最后一项不至于分母为0）\n类推得到$t_2$的目标： $$max\\ \\frac{h(t_2, X_2)}{h(t_2, X_2) + h(t_2, X_3)}$$\n$t_3$的目标： $$max\\ \\frac{h(t_3, X_3)}{h(t_3, X_3)}$$\n所以似然函数是： $$L(\\beta) = \\frac{h(t_1, X_1)}{h(t_1, X_1) + h(t_1, X_2) + h(t_1, X_3)} \\frac{h(t_2, X_2)}{h(t_2, X_2) + h(t_2, X_3)} \\frac{h(t_3, X_3)}{h(t_3, X_3)}$$\n代入$h(x)$的公式之后消掉$h_0(t)$，得到： $$L(\\beta) = \\frac{exp(\\beta · X_1)}{exp(\\beta · X_1) + exp(\\beta · X_2) + exp(\\beta · X_3)} \\frac{exp(\\beta · X_2)}{exp(\\beta · X_2) + exp(\\beta · X_3)} \\frac{exp(\\beta · X_3)}{exp(\\beta · X_3)}$$\n这里我们假设的是3个事件，再泛化到N个的情况： $$L(\\beta)=\\prod_{i=1}^{N} \\frac{\\exp \\left(\\beta \\cdot X_{i}\\right)}{\\sum_{j: t_{j} \\geq t_{i}} \\exp \\left(\\beta \\cdot X_{j}\\right)}$$\n对数似然函数： $$l(\\beta)=\\log L(\\beta)=\\sum_{i=1}^{N}\\left[\\beta \\cdot X_{i}-\\log \\left(\\sum_{j: t_{j} \\geq t_{i}} \\exp \\left(\\beta \\cdot X_{j}\\right)\\right)\\right]$$\n梯度为： $$\\frac{\\partial l(\\beta)}{\\partial \\beta}=\\sum_{i=1}^{N}\\left[\\beta-\\frac{\\sum_{j: t_{j} \\geq t_{i}} X_{j} \\cdot \\exp \\left(\\beta \\cdot X_{j}\\right)}{\\sum_{j: t_{j} \\geq t_{i}} \\exp \\left(\\beta \\cdot X_{j}\\right)}\\right]$$\n就可以采用梯度下降法来对参数进行估计\n解读结果 解得了合适的$h_0(t)$以及协变量系数之后，我们可以比较某个协变量$x_i$在不同值的时候对应的不同风险比$\\frac{x_i + 1}{x_i}$。\n$$hazard\\ ratio = \\frac{h_0(t) \\times e^{b_1x_1 + b_2x_2 + ...b_i(x_i+1) + ... b_px_p}}{h_0(t) \\times e^{b_1x_1 + b_2x_2 + ...b_ix_i + ... b_px_p}} = e^{b_i}$$\n举个例子，假如某个指标$x_i$表示年龄，那么对于年龄x和年龄x+1的人来说，死亡风险比是$e^{b_i}$，如果$b_i\u003e0$，则年龄增大，死亡风险增大。反之减小。等于0则是不起作用\n","description":"","tags":["生存分析","统计学"],"title":"生存分析基础","uri":"/posts/survival-analysis-basic/"},{"categories":["理论"],"content":"第一章：导论 概念部分 第一台通用电子计算机诞生于1946年\n计算机技术的飞速发展得益于：计算机制造技术的发展、计算机系统结构的创新\n纷纷放弃高性能转向多核，标志着系统结构的重大转折：从单纯依靠指令集并行转向开发线程级并行和数据集并行\n计算机系统的层次结构\nL6: 应用语言虚拟机\nL5: 高级语言虚拟机\nL4: 汇编语言虚拟机\nL3: 操作系统虚拟机\nL2: 传统机器级\nL1: 微程序机器级\nL1-L3通常使用解释实现（一条一条来）\nL4-L6通常使用翻译实现（全部翻译成下面一个低级再执行）\n计算机系统结构定义：计算机系统结构是程序员所看到的计算机属性，即概念性结构与功能特性，是计算机系统的\u0008软硬件的界面\n广义的系统结构：指令结构、组成和硬件\n包括：指令系统、寻址方式、数据表示、寄存器定义、中断系统、工作状态的切换、存储系统、信息保护、I/O结构等\n计算机组成：计算机系统结构的逻辑实现\n计算机实现：计算机系统结构的物理实现\n一种体系结构可以有多种组成，一种组成可以有多种物理实现\n系统结构的分类 冯氏分类法：按照最大并行度（字宽×一次能处理的字数）分 Flynn分类：按指令流和数据流的多倍性分，分为以下四类：\n①单指令流单数据流（SISD）\n②单指令流多数据流（SIMD）\n③多指令流单数据流（MISD）\n④多指令流多数据流（MIMD）\n冯诺依曼结构\n最大特点：以运算器为中心\n虚拟机：用软件实现的机器\n系列机：同一个厂家生产的具有相同\u0008系统结构但具有不同组成和实现的一系列不同型号的计算机\n兼容机：由不同厂家生产的具有相同系统结构的计算机\n软件兼容：向上兼容、向下兼容、向前兼容和向后兼容，其中向后兼容是系列机的根本特征\n模拟：用软件的方法在一台计算机上实现另一台计算机的指令集（本机要解释执行另一台机子的程序）\n仿真：\u0008用一台现有计算机上的微程序去解释实现另一台计算机的指令集（本机要实现另一台机子的指令集）\n并行性：包括同时性（同一时刻）和并发性（同一间隔）\n提高并行性的措施：时间重叠、资源重复、资源共享\n耦合度：反应计算机之间物理连接的紧密程度和交互强弱，分为紧密耦合系统和松散耦合系统\n计算机系统设计经常使用的4个定量原理\n  以经常性事件为重点\n  Amdahl定律\n  CPU性能公式\n  程序的局部性原理\n  计算部分 Amdahl定律（P7）\n$加速比 = \\frac{执行时间_{改进前}}{执行时间_{改进后}} = \\frac{1}{（ 1 - 可改进比例 ）+ \\frac{可改进比例}{部件加速比} }$ 依赖于可改进比例和部件加速比\nCPU时间（P9）\n$CPU时间 = IC \\times CPI \\times 时钟周期时间$\n$时钟周期 = \\frac{1}{f}$\n$MIPS速率 = \\frac{f}{CPI} $\n执行时间和吞吐率（P11）\n性能比较（P14）\n第二章：指令集结构 概念部分 区别不同指令集结构的主要因素是CPU中用来储存操作数的储存单元的类型，因此可以把指令集结构分为堆栈结构、累加器结构和通用寄存器结构\n寻址方式： 指一种指令集结构如何确定所要访问的数据的地址\n对指令集的基本要求： 完整性、规整性、高效率和兼容性\n指令集结构设计涉及的内容\n① 指令集功能设计，主要由CISC和RISC两种方向\n② 寻址方式设计\n③ 操作数表示和操作数类型\n④ 寻址方式的表示\n⑤ 指令集格式的设计，变长、固定长度、混合\n指令集三种编码格式：\n可变长度编码\n固定长度编码\n混合型编码\nRISC遵循的原则\n① 指令条数少而简单\n② 采用简单而又统一的指令格式\n③ 指令的执行在单个机器周期内完成\n④ 只有load和store能访问存储器\n⑤ 大多数指令采用硬连逻辑\n⑥ 强调优化编译器的作用\n⑦ 充分利用流水线\nI类指令： load、store等\n   0-5 6-10 11-15 16-31     操作码 rs rt 立即数    R类指令： ALU等\n   0-5 6-10 11-15 16-20 21-25 26-31     操作码 rs rt rd shamt funct    第三章：流水线技术 概念部分 流水线技术的概念： 把一个重复的过程分解为若干个子过程，每一个子过程用一个专门的部件来实现。多个处理过程在时间上错开依次通过各段，让每个子过程和其他过程并行，这就是流水线技术\n流水线技术的特点：\n① 流水线把一个处理过程分解为若干个子过程，每个子过程由一个专门的功能部件来实现，依靠它们的并行工作来缩短程序的执行时间\n② 流水线各段时间应该尽可能相等\n③ 流水线的每一个功能部件的后面都要有一个缓冲存储器\n④ 流水线适合于大量重复的时序过程\n⑤ 流水线需要有通过时间和排空时间\n流水线分类\n单功能流水线：只能完成一种固定功能\n多功能流水线：可以实现不同的功能\n静态流水线：同一时间各段只能按照同一种功能的连接方式工作\n动态流水线：同一时间各段可以有不同的连接，执行多种功能\n部件级流水线：把运算部件分段\n处理机级流水线：把指令的解释执行过程分段\n处理机间流水线：在处理机间流水\n线性流水线：没有反馈回路\n非线性流水线：有反馈回路\n顺序流水线：任务流出流入的顺序一致\n乱序流水线：任务流出的顺序和流入的顺序可以不一样\n解决流水线瓶颈问题的方法 细分瓶颈段、重复设置瓶颈段\n经典的五段流水线划分\n一条指令的执行过程可以划分为以下五个部分：\n取指令周期（IF）\n指令译码/读寄存器（ID）\n执行/有效地址计算（EX）\n存储器访问/分支完成（MEM）\n写回周期（WB）\n相关的概念：相关是指两条指令之间存在某种依赖关系\n相关的分类\n数据相关：指令之间有数据关联（有传递性）\n名相关：名指的是寄存器名或存储器名，数据不关联但是用了相同的名，名相关又分为反相关（一写一读）和输出相关（都写）\n控制相关：由分支指令引起的相关\n流水线冲突的概念： 对于具体的流水线而言，由于相关的存在，导致指令流的下一条指令不能在指定的时钟周期执行\n流水线冲突的分类\n结构冲突：硬件资源无法满足重叠执行的要求\n— 消除结构冲突：气泡停顿、设置独立指令数据存储器、Cache分为指令Cache和数据Cache 数据冲突：重叠执行时需要用到前面的数据\n— 数据冲突分为：写后读（RAW）、写后写（WAW）\n — 解决数据冲突：定向技术（直接从产生的地方送到需要的地方）、通过编译器指令调度解决 控制冲突：分支指令等引起的冲突\n— 解决控制冲突：冻结或排空流水线（最简单但是分支延迟大）、尽早判断分支是否成功（提前到ID段末尾）、软件方法\n减少分支延迟的静态方法\n预测分支失败\n预测分支成功\n延迟分支（在延迟槽中放入有用的指令，三种调度方法：从前调度、从目标处调入、从失败处调入）\n不采用单周期的原因：\n① 单周期效率低，不同指令需要的时钟周期不一样\n② 单周期需要重复设置部件，而多周期可以共享\n流水寄存器的作用：\n① 将各段隔开来，使之不会相互干扰\n② 保存相应段的处理结果\n③ 向后传递后面要用到的数据或控制信息\n流水线的实现（P82 - P90）\n   $IF/IR$ $IR/EX$ $EX/MEM$ $MEM/WD$     $NPC$ $NPC$ $cond$ $LMD$   $IR$ $A$ $ALU0$ $ALU0$    $B$ $B$ $IR$    $Imm$ $IR$     $IR$      计算部分 吞吐率：单位时间内流水线完成的任务数量\n$TP = \\frac{n}{T_k}$\n加速比：不用流水线所用的时间和用流水线所用时间之比\n$S = \\frac{T_s}{T_k}$\n效率：设备实际使用时间与整个运行时间之比（画图之后即为阴影面积/完整面积）\n易出大题（P61 例题3.1、P104 章后习题3.11） \u0008务必注意题目里说的是静态流水线还是动态流水线，静态流水线必须一个操作做完之后 才能开另一个功能，参见P60例3.1个P60例3.2\n第四章：指令级并行 概念部分 指令级并行概念：利用流水线使指令重叠并行执行，这种指令之间潜在并行性称为指令级并行（ILP，Instruction-Level Parallelism）\nCPI：（Cycles Per Instruction）每条指令所消耗的时钟周期数 IPC：（Instructions Per Cycle）每个时钟周期完成的指令条数\n基本程序块：一段除了入口和出口之外不包含其他分支的线性代码段（就是指中间没分支）\n循环级并行：让一个循环中的不同循环体并行执行\n开发循环级并行的技术：循环展开技术、采用向量指令和向量数据表示\n指令顺序：由源程序确定的在完全串行方式下指令的执行顺序\n正确执行程序必须保持的最关键的两个因素：数据流（数据从其产生者指令到消费者指令的实际流动）和异常行为（无论怎么改变顺序，都不影响程序中异常的发生情况）\n指令调度：通过在编译时让编译器重新组织指令顺序或者通过硬件在执行时调整指令顺序来消除冲突\n静态调度与动态调度：第三章为静态调度，第四章为动态调度，以下为二者区别： ① 静态调度发生在编译过程中，动态调度发生在运行过程中 ② 动态调度相比静态有更多优点：能够处理一些编译时不明确的相关、能够套用在其他流水线上\n精确异常和不精确异常： 精确异常：发生异常时，处理机的现场和严格按程序顺序执行时的\u0008现场相同 不精确异常：发生异常时，处理机的现场和严格按程序顺序执行时的\u0008现场不同\nTomasulo算法\n保留栈：在采用Tomasulo算法的MIPS处理器浮点部件中，在运算部件的入口设置的用来保存已经流出并等待到本功能部件执行的指令 CDB：公共数据总线\nROB\n动态分支预测技术：根据分支指令过去的表现来预测其将来的行为 BHT：分支历史表，用于记录相关分支指令最近几次的执行情况并根据此进行预测 分支目标缓冲：是一种动态分支预测技术，将执行过的成功的分支指令的地址和预测的分支目标地址记录在一个硬件表中，每次取指令时比较，达到减少分支开销的作用 前瞻执行：解决控制相关的方法，对分支指令的结果进行预测，按照这个预测结果继续后续的过程，不过指令执行的结果不是放在寄存器或存储器中，而是放在ROB缓冲器中，相应指令确认后才将结果写到寄存器或存储器 ROB：前瞻执行缓冲器\n多流出处理机的两种基本风格\n超标量：一种多指令流出技术，每个时钟周期流出的指令条数不确定，但有个上限 超长指令字：一种多指令流出技术，每个时钟周期流出的指令条数是固定的，这些指令构成一条长指令或者指令包，通过编译器静态调度\n超流水 一个时钟周期内分时流出多条指令\n计算部分 实际CPI：\n$CPI_{流水线} = CPI_{理想} + 停顿_{结构冲突} + 停顿_{数据冲突} + 停顿_{控制冲突}$\n第五章：存储系统 概念部分 人们追求的储存器特性：容量大、速度快、价格低\n走出困境的唯一方法：采用多级存储层次结构\n多级存储层次：采用不同的技术实现的存储器，处在离CPU不同位置的层次上，各存储器之间一般满足包容关系，任何一层存储器中的内容都是其下一层的储存器内容的子集。目标是达到离CPU最近的存储器的速度，最远的存储器的容量。\n“Cache-主存”与“主存-辅存”的区别\n    Cache-主存层次 主存-辅存层次     目的 为了弥补主存速度的不足 为了弥补主存容量的不足   存储管理的实现 由专用硬件实现 由软件实现   访问速度的比值 几比一 几万比一   块/页大小 几十个字节 几百到几千字节   CPU对第二级的访问方式 可以直接访问 均通过第一级   不命中时是否切换 不切换 切换到其他进程    映像规则 全相联映像：主存中的任意一块可以被放置到Cache中的任意一个位置，空间利用率最高、冲突概率最低、实现最复杂 直接映像： 主存中的\u0008每一块只能被放到Cache中唯一的位置，空间利用率最低、冲突概率最高、实现最简单 组相联映像：主存中的每一块可以被放置到Cache中唯一一组中的任何一个位置，是上面二者的折中\n查找算法 查找Cache在哪，通过查找目录表实现，目录表项与储存器块对应\n目录表\n   有效位 标识    有效位为1表示有效，标识tag标识了存放的信息存在于哪个主存块中\n主存地址\n   标识 索引 块内位移    替换算法\n随机法\nFIFO\nLRU\nLRU算法的硬件实现\n堆栈法\n比较对法\n写策略\n写直达法：执行写操作时，不仅写入Cache，而且也直接写入下一级存储器（易于实现） （不按写分配，不命中直接写入下一级而不调块）\n写回法：执行写操作时，只写入Cache。仅当Cache中相应的块被替换\u0008时，才写回主存（速度快） （按写分配，不命中时调块）\nCache对低CPI、高时钟频率的CPU来说更为重要\n改进Cache性能\u0008 包括三个方面：\n① 降低不命中率（8种）\n② 减少不命中开销（5种）\n③ 减少Cache命中时间（4种）\n三种类型的不命中 强制性不命中：首次访问就没命中 \u0008容量不命中：某些块被\u0008替换了，之后又访问了这些块（原因主要是容量小了） 冲突不命中：组相联或直接映像很多块映到了同一组（块）中，原块被替换，之后又访问了这些块\n相联度越高，冲突不命中就越少（因为每块可选的位置变多了，冲突几率下降），对强制不命中和容量不命中没什么影响\nCache\u0008容量增加，容量不命中下降，对强制性不命中没影响\n减少三种不命中的方法： 强制性不命中：增加块大小，\u0008预取（本身比例很少） 容量不命中：增加容量 冲突不命中：提高相联度（理想情况：全相联）\n降低不命中率的八种方法\n① 增加Cache块大小\n最简单，减少强制不命中，但增加了冲突不命中（因为块的个数少了），同时也会增大不命中开销\n② 增加Cache容量\n最直接，但会增加成本和命中时间\n③ 提高相联度\n会增加命中时间 （2:1Cache经验规则：容量为N的直接映像Cache的不命中率和容量为N/2的两路组相联Cache的不命中率差不多）\n④ 伪相联Cache\n访问\u0008如果命中就和直接映像一样，如果不命中就检查另一个位置是否匹配，简单的方法是将索引的最高位取反。保持命中速度和低不命中率，会让CPU流水线的设计复杂化\n⑤ 硬件预取\n指令和数据在处理器提出访问之前进行预取，由Cache之外的硬件完成，放入一个缓冲器中。预取应当利用存储器的空闲带宽，不能影响对正常不命中的处理，否则可能会降低性能\n⑥ 编译器控制的预取\n由编译器在程序中加入预取指令实现预取。每次预取需要花费一条指令的开销\n⑦ 编译器优化\n三种方法：代码和数据重组、内外循环交换、分块\n⑧ 牺牲Cache\n在Cache和下一级之间设置一个全相联小Cache来存储被替换掉的块，减少冲突不命中很有效，尤其是小容量Cache\n减少Cache不命中开销的五种方法\n① 采用两级Cache（有计算）\n② 让读不命中优先于写 会增加命中时间\n③ 写缓冲合并\n写入的数据与缓冲器已有地址比较，如果有地址匹配的就合并\n④ 请求字处理技术\n从下一级调入Cache的块只有一个字是立即需要的，称为请求字，两种方法：尽早重启动、请求字优先\n在Cache块较小或者下一条指令正好访问Cache块的另一部分时，效果不明显\n⑤ 非阻塞Cache技术\n在Cache不命中时仍允许CPU进行其他的命中访问\n减少Cache命中\u0008时间的四种方法\n① 容量小、结构简单的Cache\n增大不命中率\n② 虚拟Cache\n可以直接用虚拟地址进行访问的Cache\n③ Cache访问流水化\n把对第一级Cache的访问按流水方式组织\n④ 踪迹Cache\n存放CPU所执行过的动态序列，包含分支预测展开的指令\n地址映像机制复杂，相同的指令序列可能被重复存放，提高了Cache的空间利用率\n主存主要的性能指标：延迟和带宽\n并行主存系统：在一个访存周期内能并行访问多个储存字的存储器\n并行存储器结构包括： 单体多字存储器 多体交叉存储器\n计算部分 平均每位价格C、命中率H、平均访存时间（P155）\n$M_1{T_1, S_1, C_1}、M_2{T_2, S_2, C_2}$\nT: 平均访存时间，S: 存储容量，C: 平均每位价格\n平均每位价格 = $\\frac{M_1C_1 + M_2C_2}{M_1 + M_2}$\n命中率 = $\\frac{N_1}{N_1+N_2}$\n平均访存时间 = $HT_1 + (1-H)(T_1+T_M) = T_1 + (1-H)T_M = T_1 + FT_M$ 不命中开销 $T_M = T_2 + T_B$\nCache的容量（P163）\nCache容量 = $2^{index} \\times$ 相联度 $\\times$ 块大小\n程序执行时间\n$CPU$时间 = $IC \\times (CPI + $每条指令的平均访存次数$\\times $不命中率$ \\times $不命中开销$) \\times $时钟周期时间\n（P172例题）\n第六章：输入输出系统 概念部分 I/O系统包括：I/O设备、I/O设备与处理机的连接\n系统的响应时间：从用户输入命令开始，到得到结果所花费的时间（等于I/O系统的响应时间+CPU的处理时间）\nI/O系统的三个性能指标 可靠性：一直连续提供服务的能力，用平均无故障时间MTTF衡量，其倒数为失效率（计算时失效率可累加，倒数相加再倒） 可用性：正常工作的时间在连续两次正常服务间隔中的比例 可用性=$\\frac{MTTF}{MTTF+MTTR}$ 可信性：服务的质量（无法度量）\n磁盘阵列：使用多个磁盘的组合来代替一个大容量的磁盘 阵列的并行性包括：多个请求可以由多个盘来并行处理、一个请求访问多个块也可以多个块合作地并行处理\n各种RAID （检测盘个数是数据盘个数为8个时所需要的检测盘个数）\n   名称 描述 可容忍故障 检测盘个数 优点 缺点     RAID0 非冗余阵列，没有冗余信息 0 0 无空间开销 无纠错能力   RAID1 镜像盘，\u0008每个磁盘都有备份 1 8 计算少，快 空间开销大   RAID2 汉明纠错码位交叉 1 4 不依靠故障盘诊断 空间开销log2n   RAID3 位交叉奇偶校验磁盘阵列 1 1 空间开销小，大规模I/O带宽高 小规模I/O支持不好   RAID4 块交叉奇偶校验磁盘阵列 1 1 空间开销小，小规模I/O带宽高    RAID5 块交叉分布奇偶校验磁盘阵列 1 1 空间开销小，小规模I/O带宽高 小规模读写需要访问4次   RAID6 P+Q双校验磁盘阵列 2 2 容忍两个磁盘出错 小规模读写需要访问6次    实现盘阵列的方式\n软件方式\n阵列卡方式\n子系统方式\n通道处理机\n专门负责\u0008整个计算机的输入输出工作，通道处理机只能执行有限的一组输入输出指令\n输入输出系统的层次\nCPU-\u003e通道-\u003e设备控制器-\u003e外设\n通道的主要硬件\n寄存器\n控制逻辑\n通道的工作过程（3步）\n① 在用户程序中启动一个访管指令，由管理程序来编制一个通道程序，并启动通道\n② 通道处理机执行通道程序，完成指定的数据的输入输出工作\n③ 通道程序结束后向CPU发出中断请求\n通道的种类（3种）\n① 字节多路通道，为多台低中速外设服务，以字节交叉的方式分时轮流服务，可以包含多个子通道，每个子通道连接一台设备控制器\n② 选择通道，为多台高速外围设备服务，一段时间内只为一条高速外设独占\n③ 数组多路通道，适用于高速设备，每次选择一个高速设备后传送一个数据块，轮流为多台外围设备服务\n计算部分 通道流量分析（P238）\n字节多路通道：（连一个外设，传一个字节，再连一个外设，传一个字节...）\np台设备传输n个数据所需时间：$T_{BYTE}=(T_S+T_D)\\times p\\times n$\n最大流量：$f_{MAX-BYTE} = \\frac{1}{T_S+T_D}$\n选择通道：（一台设备的数据传输工作全部结束后通道才为另一台设备服务）\np台设备传输n个数据所需时间：$T_{SELECT}=(\\frac{T_S}{n}+T_D)\\times p\\times n$\n最大流量：$f_{MAX-SELECT} = \\frac{1}{\\frac{T_S}{n}+T_D}$\n数组多路通道：（连一个外设，传一个k个字节的数据块，再连一个外设，传一个k个字节的数据块...）\np台设备传输n个数据所需时间：$T_{BLOCK}=(\\frac{T_S}{k}+T_D)\\times p\\times n$\n最大流量：$f_{MAX-BLOCK} = \\frac{1}{\\frac{T_S}{k}+T_D}$\n第八章：多处理机 概念部分 MIMD的成为通用多处理机系统结构的选择的原因\nMIMD具有灵活性\nMIMD可以充分利用现有微处理机的性价比优势\nMIMD的分类：\n集中式共享存储器结构（CSMA、UMA、对称式共享存储器多处理机SMP）\n多个处理器共享一个集中式的物理存储器，单一主存而且主存对于各处理器而言是对等的\n分布式存储器多处理机\n存储器分布到各个处理器上，优点：\n①降低对存储器和互联网络的带宽要求\n②对本地存储器的访问延迟时间小；\n缺点：处理器之间的通信较为复杂，访问延迟大\n两种储存器系统结构\n共享地址空间：物理上分离的所有储存器作为一个统一的共享逻辑空间进行编址，不同处理器的同一个物理地址指向同一个存储单元\n独立编址：每个节点的存储器编址为一个独立的地址空间，不同\u0008处理器的地址是独立的。每一个处理器-存储器模块实际上是一台单独的计算机\n两种通信机制\n共享存储器通信机制：处理器之间通过store-load对相同存储器地址进行读写来实现的 优点：\n① 与常用的SMP通信机制兼容\n② 易于编程\n③ 数据量小时开销较低\n④ 可以采用cache来减少远程通信的频度\n消息传递通信机制：处理器之间通过发送消息来进行通信\n优点\n① 硬件简单\n② 通信是显式的\n③ 减少不当的同步带来的可能的错误\n④ 显式通信让编程者重点关注主要通信开销\nCache一致性协议\n监听式协议\n目录式协议\nCache一致性问题解决方法\n写作废协议\n写更新协议\n同时多线程技术：\n一种在多流出、动态调度的处理器上同时开发线程级并行和指令级并行的技术\n","description":"","tags":["系统结构"],"title":"计算机系统结构","uri":"/posts/computer-system-structure-1/"},{"categories":["工程"],"content":"背景 mongodb是一个基于分布式文件存储的数据库。由C++语言编写。旨在为WEB应用提供可扩展的高性能数据存储解决方案。它是一个介于关系数据库和非关系数据库之间的产品，是非关系数据库当中功能最丰富，最像关系数据库的。\n这里简单的讲一下使用方法，不涉及底层理论\n起步 Mac下安装MongoDB 之前是 brew install mongodb\n但是现在会报错：No available formula with the name “mongodb”。\n先tap一个仓库 brew tap mongodb/brew 安装mongodb社区版 brew install mongodb-community\n运行mongod 新建一个/data/db文件夹\n运行MongoDB服务 sudo mongod\n（注：macOS 10.15 Catalina无法在根目录下进行修改，可以在其他目录新建，启动服务时通过sudo mongod --dbpath=new_path/data/db指定）\n定位和启动MongoDB命令行 cd /usr/local/Cellar/mongodb/4.0.3_1/bin ./mongo\nMongoDB和关系型数据库（Oracle、MySQL等）的区别：\n   SQL MongoDB     table collection(集合)   row document(文档)   colume field(数据字段)    每个文档是一组键值对（BSON）相同的字段不需要相同的数据类型\n基本命令 show dbs 展示所有数据库\nuse xx 使用某个数据库\n使用了某个数据库后：show collections查看所有的集合\ndb.dropDatabase() 删除当前数据库\ndb.\u003ccol\u003e.drop() 删除某个集合\nmongodump -o \u003coutput_path\u003e 导出数据库 （如果设置了密码，需要通过下面的命令导出） sudo mongodump -o \u003coutput_path\u003e --authenticationDatabase admin --username \u003cdb_username\u003e --password \u003cdb_password\u003e\nmongorestore -d \u003cdbname\u003e \u003cdb_path\u003e 导入数据库\n验证 本地的还好，如果部署到服务器上，默认是无法外网访问数据库的，倘若你想访问，就得开放端口然后在mongo的配置文件里设置0.0.0.0。然后mongo默认也没有密码\n这就会产生一个很蛋疼的事，当其他人访问你服务器的ip的27017端口时，可以直接完全操作你的数据库，对于非个人弄着玩的项目，这显然是不可能接受的。\n所以我们需要增加数据库验证，这里最常见的就是增加账号密码登录，方法如下：\n1 2 3 4  //先使用adminuseadmin//创建root密码db.createUser({user:\"root\",pwd:\"password\",roles:[\"root\"]})  其他的role：\nread readWrite dbAdmin userAdmin clusterAdmin readAnyDatabase readWriteAnyDatabase userAdminAnyDatabase dbAdminAnyDatabase 1 2 3 4 5 6 7  //创建用户db.createUser({user:\"username\",pwd:\"password\",roles:[\"userAdminAnyDatabase\",\"dbAdminAnyDatabase\",\"readWriteAnyDatabase\"]})  在/etc/mongo.conf配置文件里，把auth=true前面的#去掉，让验证生效。\n重启mongodb服务sudo service mongodb restart\n之后的连接方式：mongo命令行连接： mongo admin -u username -p password\n本地GUI（如navicat）连接，设置账号密码即可\n字段操作 字段重命名：\ndb.col.update({},{$rename:{\"old_field\":\"new_field\"}},false,true) 比如：\ndb.questionnaire.update({},{$rename:{\"questionList\":\"question\"}},false,true) 字段增加： 还可以指定默认值xxx\ndb.col.update({},{$set:{new_field:'xxx'}},{multi:true}) 字段删除：\ndb.col.update({},{$unset:{'old_field':''}},false, true) db.user_questionnaire.update({},{$unset:{'inputCostEstimation':''}},false, true) 查找 列出集合信息：\ndb.\u003ccollection-name\u003e.find() 列出第一条集合的信息（按Json排版一下）：\ndb.\u003ccollection-name\u003e.findOne() db.\u003ccol-name\u003e.find({query}, {show}) query是一个查询字段。比如{\"name\":\"yicheng\"}这种形式 后面的参数决定field是否显示，比find({\"name\":\"yicheng\"}, {\"_id\": 0, \"age\": 1}) 表示_id不会显示，age会显示\n条件比较：\n   操作 描述 用法     $gt \u003e    $gte \u003e=    $lt \u003c    $lte \u003c=    $eq =    $ne !=    $in in    $nin not in     增删改\n增加：\ndb.col.insert(json) e.g.\ndb.col.insert({\"name\":\"engine\", \"age\":18}) db.articles.insert({\"title\":\"Test\",\"author\":\"engine\",\"time\":\"2020.02.27\",\"kind\":\"tech\",\"tags\":\"golang,website\",\"content\":\"This is a blog for test\",\"comment\":\"comment1\",\"view\":10,\"like\":5}) db.user.insert({\"name\":\"user1\",avatar:\"https://i.loli.net/2020/03/15/XsJjRomr1dy8u4D.png\",\"type\":0,\"score\":20,\"password\":\"123456\",\"todo\":\"\"}) 删除：\ndb.col.remove(query) 修改\ndb.mycoll.update(query, object[, upsert_bool, multi_bool]) 第一个参数是查询条件，第二个参数是修改信息，第三个是如果没找到是否相当于插入（默认为false），第四个参数是修改一个还是所有（默认为false）\ne.g.\ndb.col.update({\"name\":\"engine\"}, {$set:{\"age\":20}}) db.col.update({\"age\": {$gt: 20}}, {$set:{\"age\": 30}}, false, true) 更复杂的逻辑：\ndb.getCollection('participant').find().forEach( function(item){ db.getCollection('participant').update({\"_id\":item._id},{$set:{\"modifyTimes\": 3}}) } )  数据类型 描述  String字符串。存储数据常用的数据类型。在 MongoDB 中，UTF-8 编码的字符串才是合法的。  Integer整型数值。用于存储数值。根据你所采用的服务器，可分为 32 位或 64 位。  Boolean布尔值。用于存储布尔值（真/假）。  Double双精度浮点值。用于存储浮点值。  Min/Max keys将一个值与 BSON（二进制的 JSON）元素的最低值和最高值相对比。  Array用于将数组或列表或多个值存储为一个键。  Timestamp时间戳。记录文档修改或添加的具体时间。  Object用于内嵌文档。  Null用于创建空值。  Symbol符号。该数据类型基本上等同于字符串类型，但不同的是，它一般用于采用特殊符号类型的语言。 Date日期时间。用 UNIX 时间格式来存储当前日期或时间。你可以指定自己的日期时间：创建 Date 对象，传入年月日信息。  Object ID对象 ID。用于创建文档的 ID。  Binary Data二进制数据。用于存储二进制数据。 Code代码类型。用于在文档中存储 JavaScript 代码。 Regular expression正则表达式类型。用于存储正则表达式。  更新某个字段的值\n1 2 3 4 5  db.getCollection('participant').find().forEach( function(item){ db.getCollection('participant').update({\"_id\":item._id},{$set:{\"modifiTimes\": 2}}) } )   Golang的MongoDB接口：mgo 简单的使用\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29  type Person struct { Name string Phone string } func main() { // mgo.Dial核心函数，由url新建一个session \tsession, err := mgo.Dial(\"mongodb://127.0.0.1:27017/\") if err != nil { panic(err) } defer session.Close() // Optional. Switch the session to a monotonic behavior. \tsession.SetMode(mgo.Monotonic, true) // c就连到了对应的collection \tc := session.DB(\"test\").C(\"people\") // 插入数据 \terr = c.Insert(\u0026Person{\"Ale\", \"+55 53 8116 9639\"}, \u0026Person{\"Cla\", \"+55 53 8402 8510\"}) if err != nil { log.Fatal(err) } result := Person{} // result是一个查询结果 \terr = c.Find(bson.M{\"name\": \"Ale\"}).One(\u0026result) if err != nil { log.Fatal(err) } fmt.Println(\"Phone:\", result.Phone) }   ","description":"","tags":["MongoDB","Database"],"title":"Mongodb初级教程","uri":"/posts/mongodb-basics/"},{"categories":["工程"],"content":"Django项目写好了，最后一步就是部署(deployment)，部署十分关键，只有部署在服务器上，别人才能从互联网上通过ip地址或域名直接访问到你的网页。\n第一步是购买vps（Virtual Private Server 虚拟服务器），这个很简单而且网上教程一大把，这里就不详述，我在vultr购买的海外服务器，这样不用浪费时间去备案了，vultr的一大特色就是按时长收费，如果你的vps出了什么问题，可以随时关停，并且它还支持微信支付宝，价格也很便宜。 （vultr官网） \nDjango的本地预览十分方便，一行python manage.py runserver就能搞定，但部署上线可没有这么简单。因为网上关于Django部署的教程都很杂乱，当时部署的时候就踩了很多很多坑，为了给之后一个参考，我又重新部署了一次，来记录详细的过程。\n相关软件版本： Django 2.1.3 Python 3.6.6 nginx 1.14.0 uwsgi 2.0.17.1\n服务器： Ubuntu-server 18.04\n准备工作 首先打开ssh软件，Xshell、Putty什么的都行，通过vultr上vps详情页上给的ip和root密码连接到这台vps。\n刚拿到的船新Linux，第一步先给它来个更新:\n1 2  sudo apt-get update sudo apt-get upgrade   建议使用非root用户，部署时最好使用python虚拟环境，具体操作不是本文的重点，便不赘述了\n系统自带Python3.6、vim和git，所以不用装\n安装python3-pip、python3-setuptools、gcc、python3-dev、wheel： （缺一不可，不然之后用pip安装uwsgi会有各种各样的报错）\n1  sudo apt-get install python3-pip python-setuptools python3-dev wheel   放置Django项目 直接在服务器端用vim什么的写Django当然可取（虽然会很酸爽），但更多的时候我们是在本地写好了Django项目，要把它挪到服务器上。\n在传输之前，要做一些工作：\n先更改一下setting.py里的ALLOWED_HOSTS，把服务器的ip加进去，有域名的话顺便把域名也加进去，要不然之后会无法加载Django项目\n在本地的Python虚拟环境上使用pip freeze \u003e requirements.txt,生成一个txt文件，里面是需要的Python库以及其版本，之后一并传给服务器\n传输文件到服务器的方法非常之多：可以使用Xshell自带的文件传输，也可以使用linux命令scp或安装更直观的lrzsz，或者使用本地的FileZilla、Winscp等软件，当然万能的git也很不错。\n不过考虑到之后这个web项目之后也要修改，用上面的方法感觉都不是特别方便，介绍一个非常好用的方法，那就是使用Pycharm自带的deployment功能，可以实现实时上传以及下载文件，很是方便。\n在Tools-\u003eDeployment-\u003eConfiguration中配置好与自己服务器的连接，IP地址、用户名、密码以及对应项目路径\n    在Settings-\u003eProject Interpreter里把项目解释器更改为服务器里的Python，mappings里填写两边项目的目录，再加一条manage.py的映射\n  apply之后Pycharm右下角会出现上传进度条，会有点慢，喝杯茶等一段时间即可\n传输完毕后会发现本地的项目已经全部上传到服务器了\n但这个毕竟不是这篇文章的重点，不重点介绍，遇到了什么问题可以留言或者私信我。\n最后别忘了把requirements.txt上传到服务器，用pycharm的话只要直接把文件拖进本地项目目录，Pycharm就会自动帮我们上传到服务器。\n在服务器上使用pip install -r requirements.txt来安装必要的Python packages 安装与配置uwsgi 使用pip3安装uwsgi（注意是pip安装，不是apt-get，否则之后会各种报错）\n1  pip3 install uwsgi   下面来试一下uwsgi是否好使： 找个位置新建一个py文件，就叫uwsgi_test.py好了，然后用vim打开\n1 2  touch uwsgi_test.py vim uwsgi_test.py   写入以下内容：\n1 2 3  def application(env, start_response): start_response('200 OK', [('Content-Type','text/html')]) return [b\"Hello Uwsgi\"]   wq保存退出（vim的基本操作不赘述，网上教程一大把）\n然后输入以下命令启动uwsgi，把这个部署到某个端口，以9090端口为例\nuwsgi --http :9090 --wsgi-file uwsgi_test.py 这时会出现spawned uWSGI worker 1 (and the only) (pid: 11812, cores: 1)  找个浏览器，访问http://\u003c你的服务器ip\u003e:9090/，不出意外的话你会看到Hello Uwsgi的字样，说明uwsgi能正常运行。\n在项目目录下新建uwsgi.ini文件并编辑加入以下内容：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23  [uwsgi] # 直接访问uwsgi的端口号，绕过nginx http = :8010 # 转发给nginx的端口号 socket = 127.0.0.1:8001 # 是否使用主线程 master = true # 项目的绝对路径 chdir = /var/www/\u003cPROJECT_NAME\u003e/ # Django项目wsgi.py文件的相对路径 wsgi-file = \u003cPROJECT_NAME\u003e/wsgi.py # 进程数 processes = 4 # 每个进程的线程数 threads = 2 # 监听端口 stats = 127.0.0.1:9191 # 每次退出时是否清理环境配置 vacuum = true # 目录中一旦有文件被改动就自动重启 touch-reload = /var/www/my_site # 存放日志 daemonize = /var/www/my_site/uWSGI.log   [uwsgi] # 直接访问uwsgi的端口号，绕过nginx http = :8010 # 转发给nginx的端口号 socket = 127.0.0.1:8001 # 是否使用主线程 master = true # 项目的绝对路径 chdir = /var/www/bangumi_project/ # Django项目wsgi.py文件的相对路径 wsgi-file = bangumi_project/wsgi.py # 进程数 processes = 4 # 每个进程的线程数 threads = 2 # 监听端口 stats = 127.0.0.1:9191 # 每次退出时是否清理环境配置 vacuum = true # 目录中一旦有文件被改动就自动重启 touch-reload = /var/www/bangumi_project # 存放日志 daemonize = /var/www/bangumi_project/uWSGI.log 加入uwsgi.ini的目的是使让uwsgi对接Django项目的启动变得更简便，否则就得在终端敲很长的代码\n有了uwsgi.ini我们只需要输入uwsgi --ini uwsgi.ini就可以运行，浏览器输入ip地址加:8010端口（先绕过nginx因为还没配置呢），发现可以显示我们的项目了，这时css等静态文件可能没获取到，别急\n安装和配置nginx 先sudo apt-get install nginx安装nginx，安装后nginx会自动启动，默认端口为80端口，浏览器输入ip地址加:80，可以看到\"Welcome to nginx\"的欢迎界面\n把/etc/nginx/目录下的uwsgi_params复制到项目目录下，也可以直接项目目录下新建uwsgi_params文件，写入以下内容：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  uwsgi_param QUERY_STRING $query_string; uwsgi_param REQUEST_METHOD $request_method; uwsgi_param CONTENT_TYPE $content_type; uwsgi_param CONTENT_LENGTH $content_length; uwsgi_param REQUEST_URI $request_uri; uwsgi_param PATH_INFO $document_uri; uwsgi_param DOCUMENT_ROOT $document_root; uwsgi_param SERVER_PROTOCOL $server_protocol; uwsgi_param REQUEST_SCHEME $scheme; uwsgi_param HTTPS $https if_not_empty; uwsgi_param REMOTE_ADDR $remote_addr; uwsgi_param REMOTE_PORT $remote_port; uwsgi_param SERVER_PORT $server_port; uwsgi_param SERVER_NAME $server_name;   前往/etc/nginx/目录，查看nginx.conf（nginx基础配置），发现里面有这么两行，意思就是包含conf.d文件夹中所有以conf后缀的配置和site-enabled文件夹中的内容\ninclude /etc/nginx/conf.d/*.conf; include /etc/nginx/sites-enabled/*; 我们不更改nginx.conf基础配置，只需要修改conf.d目录下的conf文件即可，进入conf.d文件夹，修改default.conf文件，没有的话就新建一个（还可以修改site-enabled/default或者sites-available/default，效果都一样的）\n然后写入以下内容：（务必根据自己的情况做相应更改）\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29  upstream django { server 127.0.0.1:8001; } server { # 监听端口，可改  listen 80; # 修改为你的ip或者域名  server_name 1.2.3.4; # 编码方式  charset utf-8; # 日志记录，可选  access_log /var/www/\u003cPROJECT_NAME\u003e/nginx_access.log; error_log /var/www/\u003cPROJECT_NAME\u003e/nginx_error.log; # 静态文件所在目录（自行修改）  location /static { alias /var/www/my_site/blog/static; } # 媒体文件所在目录（自行修改）  #location /media {  # alias /home/www/djangotest/Hello/media; # 媒体文件所在文件夹  #}  location / { include /var/www/\u003cPROJECT_NAME\u003e/uwsgi_params; uwsgi_pass django; } }   运行service nginx restart\n如果报错nginx.service failed because the control process exited with error code，那么运行一下nginx -t -c /etc/nginx/nginx.conf，可以很容易的找到问题在哪。\n浏览器输入ip地址，如果发现看到的还是\"Welcome to nginx\"，这个是因为在nginx.conf中还include了一个sites-enabled/*，它覆盖了我们在default.conf中的配置，可以干脆直接去nginx.conf里把include /etc/nginx/sites-enabled/*;这一行删掉，或者调换两行位置。 如果当时直接修改的sites-available或者sites-enabled中的default，就不会有这个问题\n这时再访问我们的ip，就能看到自己在本地搭建的Django项目了，因为在配置nginx的时候写入了static的路径，所以css什么的都加载进来了。\n至此nginx配置完毕\n后续工作 服务器上的Django还没有执行数据库迁移与管理员创建，所以记得执行\n1 2  python manage.py makemigrations python manage.py migrate   以及\n1  python manage.py createsuperuser   每次有更新时都要重载uwsgi与nginx才能生效，为了方便uwsgi的重载，在项目目录下新建一个uwsgi文件夹，然后在里面新建两个文件:uwsgi.pid（用于重载停止等操作）和uwsgi.status（用于查看状态）\n修改uwsgi.ini，把原先的stats那行删掉，下面加上这两行：\nstats=%(chdir)/uwsgi/uwsgi.status pidfile=%(chdir)/uwsgi/uwsgi.pid 这样如果项目有更新，就可以使用这两个命令来分别重载uwsgi和nginx了\nuwsgi --reload uwsgi/uwsgi.pid systemctl reload nginx.service 至此我们的Django项目就部署完成了\n","description":"","tags":["Django","Linux","nginx"],"title":"Django2+nginx+uwsgi+Ubuntu部署记录","uri":"/posts/django2+nginx+uwsgi+ubuntu%E9%83%A8%E7%BD%B2%E8%AE%B0%E5%BD%95/"},{"categories":["生活"],"content":"前年的时候，也就是17年年底，鉴于最低配的苏菲4性能比较羸弱，可怜的4G内存开个Chrome和Office就能吃满，分分钟卡爆，于是原来的电脑出掉了，准备更换一台高性能的电脑。\n当时还在原专业读土木，某些软件只有Windows才有，上mbp显然是没想过的，另外宿舍用台式机也比较麻烦，所以只能选Windows笔记本。\n预算其实并不是非常充足，但是内心有一种强烈的想法：买个配置够用的电脑多用几年。说实话五千块的电脑用三四年，真心不如一万的电脑用七八年。\n卡顿是对时间的浪费，崩溃是对心血的亵渎。\n一台电脑，配置固然是核心，但对我这种强迫症而言，工业设计也是一个重要因素。于是最后决定在XPS 15或者precision 5510中选择，它们的模具是一样的，主要区别在于其显卡和定位，前者是搭载游戏显卡，定位是超极本；后者搭载图形显卡，定位是移动图形工作站。\n因为不玩大型游戏，所以最后选择了海淘一款官翻的precision 5510移动图形工作站，虽然买的价格不到国行新机的一半，但还是非常高。最后咬咬牙还是买了，事实证明这台precision 5510确实值这个价。\n *图片来源：Amazon.com* 主要部件如下：\n  intel i7 6820HQ 处理器\n  16GB 镁光 DDR4 内存条\n  SK hynix 512GB 固态硬盘\n  夏普 SHP1476 屏幕\n  Nvidia Quadro M1000M 图形显卡\n  Windows10 pro 系统\n  使用这台工作站，跑了一年多Windows，总的体验当然是很用得很舒服，基本没出现过卡顿的情况，除了Ae渲染时风扇会狂转，其他时候完全无压力啊。win10本身尤其是UWP应用对4k触摸屏还是比较友好的（除了Windows的遗留win7风格界面），但是一些比较老旧或者小众的软件根本没有适配4k，导致那些软件要么字体非常小，看瞎了，要么强行靠缩放来保持尺寸，导致图标和字体发虚严重。4k在windows下的体验只能打80分。\n最开始没想到要装黑苹果，只是偶然在B站看到了科技美学的XPS 15顶配黑苹果评测，瞬间被折服了: 「科技美学」戴尔XPS15顶配版体验测评 黑苹果系统安装\n当海舟用手开始在屏幕上拖视频块来做FCP剪辑时，弹幕已经沸腾了，就算是顶配定制版MacBook Pro也没有触摸屏，不管原因是为了与ipad的交互区分开，以免造成产品线混淆（正常人理解）还是考虑到抬手触屏不方便 触控板万岁！（果粉理解）亦或只是为了节约屏幕成本 赚更多的钱（果黑理解），事实还是：mbp就是没有配置触摸屏。\n然而有趣的是，Mac OS系统和软件不仅支持高分辨率屏幕触摸操作，甚至比win10还支持的更好。\n前几天通过针大的教程Dell Precision 5510 Mojave Clover分享，花了两三天就比较顺利的装上了黑苹果，版本为Mojave 10.14.3，针大不仅写了很详细的教程和分享clover等文件，还通过私信解决了我的很多疑问，非常感激。如果有同样的precision5510想安装黑苹果的，可以去参考针大的这篇文章。\n因为一些软件只能在win上运行，因此装的是双系统，512GB的SSD给Mac OS分了320GB，给win10留了剩下的一百来G。\n下面对precision m5510 4k版黑苹果的各个方面做一个简单的小结，与mbp相比的优势与不足，以及黑苹果的完整程度：\n屏幕 MacBook Pro 15.4英寸的屏幕是 2880 x 1800 P3广色域 Retina视网膜屏 Precision5510 / XPS15 4k版的是 3840 x 2160 100%AdobeRGB UHD 康宁大猩猩玻璃触屏\n单论屏幕素质，Retina的校色更准确，观感更舒适，而Dell的镜面屏分辨率更高，观感更鲜艳。这两款屏幕的素质在笔记本中绝对都在第一梯队，是设计人员的福音，两款屏幕属于神仙打架。\n但有着触摸屏和Mac OS的加持，Precision 4k版本实现了mbp没有加入的触屏功能，感觉像是体验到了未来的mbp。\n触控板 mbp的触控板有目共睹，触感和手势交互甩了普通PC一条街，precision/xps的触控板在win本中属于很优秀的，说实话触感虽然和mac是两种感觉，但也十分不错，可喜的是在黑苹果中precision/xps也各种手势操作能够有效执行，比如三指上下划、五指缩放等等，虽然还有一些距离但已经有些接近mbp了。但是前者触控板做工任然有一些瑕疵，看到很多反馈是有一部分比例的x/p本触控板出现轻微的下凹或上凸。\nMac可以不需要鼠标，只用触控板做大部分工作，但是我尝试了一下搭载Mac OS的5510，还是不能完全脱离鼠标。\n键盘 precision/xps的键盘键程太短，说实话敲起来完全没有感觉，而且品控有的有些问题，我的两个Shift键都失灵了，扣下键帽也按不动就很难受，必须接着外接机械键盘，这样确实没问题而且敲得爽，但是外接键盘背来背去很不方便。\n续航 续航是4k版本的大短板，第一次使用这台电脑时，不插电两个小时后电脑就没电自动关机了，我的内心是绝望的。一方面是电池本身不足，另一方面是屏幕和显卡等（尤其是屏幕）耗电严重，我的这台5510就算开着屏幕什么软件也不运行，也就只能撑两个来小时；中大型软件开几个，80分钟内就能耗光全部电量。平常使用一直都得插着电，电脑要背走，充电器必须跟着，没有插座的地方完全不敢开机。\n也许是我的这台情况过于严重了，其他的4k屏5510可能要好一些，不过也好不到太多，续航确实是硬伤。\n相关驱动 intel的网卡无法驱动，需要去某宝单独购买，拆机后自动识别和正常苹果没区别。丽台M1000M图形显卡无法驱动，不过i7 6820hq的集显就够用了，只要不玩大游戏就好说，设计软件无压力。其他的驱动基本都可以正常运行。\n总结 总的说来，precision 5000系列/XPS 15可能是win阵营中，最适合安装黑苹果的机器之一，尤其是4k版本，触摸屏的加持更是更够体验到Mac OS别样的功能，4k下Mac的体验也比win10强太多了。\n另外这个系列的模具工艺质量与工业设计很出色，也是PC本中少见的设计感与mbp有的一拼的产品。\n以上便是我的Precision 5510安装黑苹果后的使用体验，若有任何疑问或错误请联系我，谢谢。\n两天时间陆陆续续装了一些应用上去（未完待续），下一篇不鸽的话会写Mac的应用推荐与资源分享，不见不散\n","description":"","tags":["黑苹果","折腾"],"title":"Precision5510 黑苹果使用体验","uri":"/posts/precision5510-hackintosh/"}]
